{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 17:38:43.909917: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-08 17:38:44.016422: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from src.cv import *\n",
    "from src.utils import plot_cv_indices\n",
    "path='data/'#please change to your directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "Data is already preprocessed by zscore method (group by date, symbol), no clip and dropna is applied\n",
    "\n",
    "\"date\"&\"symbol\" is main key of the dataset, y is the real return (oneday vwap to vwap return), and x_1 to x_100 are features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(path+\"panel_zscore.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>x_5</th>\n",
       "      <th>x_6</th>\n",
       "      <th>x_7</th>\n",
       "      <th>x_8</th>\n",
       "      <th>x_9</th>\n",
       "      <th>...</th>\n",
       "      <th>x_81</th>\n",
       "      <th>x_82</th>\n",
       "      <th>x_83</th>\n",
       "      <th>x_84</th>\n",
       "      <th>x_85</th>\n",
       "      <th>x_86</th>\n",
       "      <th>x_87</th>\n",
       "      <th>x_88</th>\n",
       "      <th>x_89</th>\n",
       "      <th>x_90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.313137e+06</td>\n",
       "      <td>4.313138e+06</td>\n",
       "      <td>4.253311e+06</td>\n",
       "      <td>4.299255e+06</td>\n",
       "      <td>4.307458e+06</td>\n",
       "      <td>3.661286e+06</td>\n",
       "      <td>3.906299e+06</td>\n",
       "      <td>4.301681e+06</td>\n",
       "      <td>4.284184e+06</td>\n",
       "      <td>4.255813e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>4.301554e+06</td>\n",
       "      <td>4.307431e+06</td>\n",
       "      <td>4.301594e+06</td>\n",
       "      <td>4.310315e+06</td>\n",
       "      <td>4.301698e+06</td>\n",
       "      <td>4.255437e+06</td>\n",
       "      <td>4.257755e+06</td>\n",
       "      <td>4.310291e+06</td>\n",
       "      <td>4.307459e+06</td>\n",
       "      <td>8.438670e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.126310e-04</td>\n",
       "      <td>3.037473e-17</td>\n",
       "      <td>-4.359578e-16</td>\n",
       "      <td>2.231442e-16</td>\n",
       "      <td>5.181451e-16</td>\n",
       "      <td>-1.703138e-17</td>\n",
       "      <td>5.177433e-18</td>\n",
       "      <td>3.269594e-18</td>\n",
       "      <td>1.326216e-17</td>\n",
       "      <td>4.392108e-19</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.099740e-18</td>\n",
       "      <td>-8.151732e-18</td>\n",
       "      <td>-7.496139e-18</td>\n",
       "      <td>1.089562e-18</td>\n",
       "      <td>-2.335994e-18</td>\n",
       "      <td>1.075374e-18</td>\n",
       "      <td>-1.144653e-18</td>\n",
       "      <td>-1.675874e-18</td>\n",
       "      <td>5.782300e-18</td>\n",
       "      <td>-4.815654e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.610982e-02</td>\n",
       "      <td>9.998593e-01</td>\n",
       "      <td>9.998595e-01</td>\n",
       "      <td>9.998593e-01</td>\n",
       "      <td>9.998593e-01</td>\n",
       "      <td>9.998354e-01</td>\n",
       "      <td>9.998447e-01</td>\n",
       "      <td>9.998593e-01</td>\n",
       "      <td>9.998595e-01</td>\n",
       "      <td>9.998596e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>9.998593e-01</td>\n",
       "      <td>9.998593e-01</td>\n",
       "      <td>9.998593e-01</td>\n",
       "      <td>9.998593e-01</td>\n",
       "      <td>9.998593e-01</td>\n",
       "      <td>9.998596e-01</td>\n",
       "      <td>9.998597e-01</td>\n",
       "      <td>9.998593e-01</td>\n",
       "      <td>9.998593e-01</td>\n",
       "      <td>9.992917e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.800885e-01</td>\n",
       "      <td>-3.638261e+00</td>\n",
       "      <td>-6.959706e+00</td>\n",
       "      <td>-4.937194e+00</td>\n",
       "      <td>-8.152284e+00</td>\n",
       "      <td>-1.962009e+00</td>\n",
       "      <td>-1.724462e+01</td>\n",
       "      <td>-4.198245e+01</td>\n",
       "      <td>-5.063175e-01</td>\n",
       "      <td>-6.676551e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.210433e+00</td>\n",
       "      <td>-2.555749e+00</td>\n",
       "      <td>-2.714525e+00</td>\n",
       "      <td>-2.305077e+00</td>\n",
       "      <td>-2.506787e+00</td>\n",
       "      <td>-6.270234e+01</td>\n",
       "      <td>-5.836067e+01</td>\n",
       "      <td>-6.617847e+01</td>\n",
       "      <td>-1.143625e+01</td>\n",
       "      <td>-1.401893e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.299463e-02</td>\n",
       "      <td>-6.862480e-01</td>\n",
       "      <td>-6.693071e-01</td>\n",
       "      <td>-5.752912e-01</td>\n",
       "      <td>-6.326464e-01</td>\n",
       "      <td>-7.476657e-01</td>\n",
       "      <td>-5.481983e-01</td>\n",
       "      <td>4.552859e-02</td>\n",
       "      <td>-2.830659e-01</td>\n",
       "      <td>3.251683e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.943339e-01</td>\n",
       "      <td>-5.776927e-01</td>\n",
       "      <td>-5.915130e-01</td>\n",
       "      <td>-6.500867e-01</td>\n",
       "      <td>-6.678125e-01</td>\n",
       "      <td>3.600150e-02</td>\n",
       "      <td>4.605408e-02</td>\n",
       "      <td>3.866734e-02</td>\n",
       "      <td>-3.713897e-01</td>\n",
       "      <td>-3.676833e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-7.951443e-04</td>\n",
       "      <td>-1.426867e-02</td>\n",
       "      <td>7.930003e-03</td>\n",
       "      <td>2.684213e-02</td>\n",
       "      <td>1.308213e-01</td>\n",
       "      <td>-3.012065e-01</td>\n",
       "      <td>1.982609e-02</td>\n",
       "      <td>2.791854e-01</td>\n",
       "      <td>-2.295802e-01</td>\n",
       "      <td>1.088974e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.931140e-01</td>\n",
       "      <td>-2.223638e-01</td>\n",
       "      <td>-2.330117e-01</td>\n",
       "      <td>-2.109718e-01</td>\n",
       "      <td>-1.712421e-01</td>\n",
       "      <td>8.203304e-02</td>\n",
       "      <td>7.792664e-02</td>\n",
       "      <td>1.057867e-01</td>\n",
       "      <td>2.720713e-01</td>\n",
       "      <td>2.380201e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.192325e-02</td>\n",
       "      <td>6.703282e-01</td>\n",
       "      <td>6.744865e-01</td>\n",
       "      <td>6.212844e-01</td>\n",
       "      <td>7.786921e-01</td>\n",
       "      <td>4.980246e-01</td>\n",
       "      <td>5.698795e-01</td>\n",
       "      <td>3.596286e-01</td>\n",
       "      <td>-8.906027e-02</td>\n",
       "      <td>1.814041e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>3.337790e-01</td>\n",
       "      <td>2.708023e-01</td>\n",
       "      <td>2.840730e-01</td>\n",
       "      <td>3.955702e-01</td>\n",
       "      <td>4.584224e-01</td>\n",
       "      <td>1.206110e-01</td>\n",
       "      <td>1.020532e-01</td>\n",
       "      <td>1.682710e-01</td>\n",
       "      <td>6.374296e-01</td>\n",
       "      <td>6.498236e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.459789e-01</td>\n",
       "      <td>2.829913e+01</td>\n",
       "      <td>5.148979e+00</td>\n",
       "      <td>4.794276e+00</td>\n",
       "      <td>2.193066e+00</td>\n",
       "      <td>1.263523e+01</td>\n",
       "      <td>1.387178e+01</td>\n",
       "      <td>5.753373e-01</td>\n",
       "      <td>5.459032e+01</td>\n",
       "      <td>3.505103e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>3.367481e+01</td>\n",
       "      <td>1.652724e+01</td>\n",
       "      <td>1.643988e+01</td>\n",
       "      <td>1.405882e+01</td>\n",
       "      <td>1.464965e+01</td>\n",
       "      <td>5.189612e-01</td>\n",
       "      <td>4.127793e-01</td>\n",
       "      <td>6.140990e-01</td>\n",
       "      <td>2.343313e+00</td>\n",
       "      <td>2.353190e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  y           x_1           x_2           x_3           x_4  \\\n",
       "count  4.313137e+06  4.313138e+06  4.253311e+06  4.299255e+06  4.307458e+06   \n",
       "mean   1.126310e-04  3.037473e-17 -4.359578e-16  2.231442e-16  5.181451e-16   \n",
       "std    2.610982e-02  9.998593e-01  9.998595e-01  9.998593e-01  9.998593e-01   \n",
       "min   -2.800885e-01 -3.638261e+00 -6.959706e+00 -4.937194e+00 -8.152284e+00   \n",
       "25%   -1.299463e-02 -6.862480e-01 -6.693071e-01 -5.752912e-01 -6.326464e-01   \n",
       "50%   -7.951443e-04 -1.426867e-02  7.930003e-03  2.684213e-02  1.308213e-01   \n",
       "75%    1.192325e-02  6.703282e-01  6.744865e-01  6.212844e-01  7.786921e-01   \n",
       "max    3.459789e-01  2.829913e+01  5.148979e+00  4.794276e+00  2.193066e+00   \n",
       "\n",
       "                x_5           x_6           x_7           x_8           x_9  \\\n",
       "count  3.661286e+06  3.906299e+06  4.301681e+06  4.284184e+06  4.255813e+06   \n",
       "mean  -1.703138e-17  5.177433e-18  3.269594e-18  1.326216e-17  4.392108e-19   \n",
       "std    9.998354e-01  9.998447e-01  9.998593e-01  9.998595e-01  9.998596e-01   \n",
       "min   -1.962009e+00 -1.724462e+01 -4.198245e+01 -5.063175e-01 -6.676551e+01   \n",
       "25%   -7.476657e-01 -5.481983e-01  4.552859e-02 -2.830659e-01  3.251683e-02   \n",
       "50%   -3.012065e-01  1.982609e-02  2.791854e-01 -2.295802e-01  1.088974e-01   \n",
       "75%    4.980246e-01  5.698795e-01  3.596286e-01 -8.906027e-02  1.814041e-01   \n",
       "max    1.263523e+01  1.387178e+01  5.753373e-01  5.459032e+01  3.505103e-01   \n",
       "\n",
       "       ...          x_81          x_82          x_83          x_84  \\\n",
       "count  ...  4.301554e+06  4.307431e+06  4.301594e+06  4.310315e+06   \n",
       "mean   ... -9.099740e-18 -8.151732e-18 -7.496139e-18  1.089562e-18   \n",
       "std    ...  9.998593e-01  9.998593e-01  9.998593e-01  9.998593e-01   \n",
       "min    ... -2.210433e+00 -2.555749e+00 -2.714525e+00 -2.305077e+00   \n",
       "25%    ... -5.943339e-01 -5.776927e-01 -5.915130e-01 -6.500867e-01   \n",
       "50%    ... -1.931140e-01 -2.223638e-01 -2.330117e-01 -2.109718e-01   \n",
       "75%    ...  3.337790e-01  2.708023e-01  2.840730e-01  3.955702e-01   \n",
       "max    ...  3.367481e+01  1.652724e+01  1.643988e+01  1.405882e+01   \n",
       "\n",
       "               x_85          x_86          x_87          x_88          x_89  \\\n",
       "count  4.301698e+06  4.255437e+06  4.257755e+06  4.310291e+06  4.307459e+06   \n",
       "mean  -2.335994e-18  1.075374e-18 -1.144653e-18 -1.675874e-18  5.782300e-18   \n",
       "std    9.998593e-01  9.998596e-01  9.998597e-01  9.998593e-01  9.998593e-01   \n",
       "min   -2.506787e+00 -6.270234e+01 -5.836067e+01 -6.617847e+01 -1.143625e+01   \n",
       "25%   -6.678125e-01  3.600150e-02  4.605408e-02  3.866734e-02 -3.713897e-01   \n",
       "50%   -1.712421e-01  8.203304e-02  7.792664e-02  1.057867e-01  2.720713e-01   \n",
       "75%    4.584224e-01  1.206110e-01  1.020532e-01  1.682710e-01  6.374296e-01   \n",
       "max    1.464965e+01  5.189612e-01  4.127793e-01  6.140990e-01  2.343313e+00   \n",
       "\n",
       "               x_90  \n",
       "count  8.438670e+05  \n",
       "mean  -4.815654e-17  \n",
       "std    9.992917e-01  \n",
       "min   -1.401893e+01  \n",
       "25%   -3.676833e-01  \n",
       "50%    2.380201e-01  \n",
       "75%    6.498236e-01  \n",
       "max    2.353190e+00  \n",
       "\n",
       "[8 rows x 91 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.date=pd.to_datetime(data.date)\n",
    "data.symbol=data.symbol.astype('str').apply(lambda x:x.zfill(6))\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('x_90', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date         0\n",
      "symbol       0\n",
      "y            1\n",
      "x_1          0\n",
      "x_2       2986\n",
      "          ... \n",
      "x_85         0\n",
      "x_86      1231\n",
      "x_87         1\n",
      "x_88         1\n",
      "x_89         0\n",
      "Length: 92, dtype: int64\n",
      "date         0\n",
      "symbol       0\n",
      "y            0\n",
      "x_1          0\n",
      "x_2       1368\n",
      "          ... \n",
      "x_85         0\n",
      "x_86         0\n",
      "x_87         2\n",
      "x_88        23\n",
      "x_89         0\n",
      "Length: 92, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_data = data[(data['date'] >= '2018-04-01') & (data['date'] <= '2021-01-01')]\n",
    "eval_data = data[data['date'] >= '2021-01-01']\n",
    "print(train_data.isnull().sum())\n",
    "def fillna_group(group):\n",
    "    return group.fillna(method='ffill', axis=0).fillna(method='bfill', axis=0)\n",
    "\n",
    "train_data = train_data.groupby('symbol').apply(fillna_group)\n",
    "train_data = train_data.dropna()\n",
    "\n",
    "X = train_data.drop(['y', 'date', 'symbol'], axis=1).values\n",
    "y = train_data['y'].values\n",
    "\n",
    "eval_data = data[data['date'] >= '2021-01-01']\n",
    "print(eval_data.isnull().sum())\n",
    "\n",
    "eval_data = eval_data.groupby('symbol').apply(fillna_group)\n",
    "eval_data = eval_data.dropna()\n",
    "\n",
    "eval_dates = eval_data.date\n",
    "eval_X = eval_data.drop(['y', 'date', 'symbol'], axis=1).values\n",
    "eval_y = eval_data['y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(train_data.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n",
    "\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "\n",
    "    cmap_cv = plt.cm.coolwarm\n",
    "\n",
    "    jet = plt.cm.get_cmap('jet', 256)\n",
    "    seq = np.linspace(0, 1, 256)\n",
    "    _ = np.random.shuffle(seq)  # inplace\n",
    "    cmap_data = ListedColormap(jet(seq))\n",
    "\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
    "                   c=indices, marker='_', lw=lw, cmap=cmap_cv,\n",
    "                   vmin=-.2, vmax=1.2)\n",
    "\n",
    "    # Plot the data classes and groups at the end\n",
    "    # ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n",
    "    #            c=y, marker='_', lw=lw, cmap=plt.cm.Set3)\n",
    "\n",
    "    ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n",
    "               c=group, marker='_', lw=lw, cmap=cmap_data)\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits)) + ['day'] # 'target',\n",
    "    ax.set(yticks=np.arange(n_splits + 1) + .5, yticklabels=yticklabels,\n",
    "           xlabel='Sample index', ylabel=\"CV iteration\",\n",
    "           ylim=[n_splits + 1.2, -.2], xlim=[0, len(y)])\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1445357/252170138.py:10: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  jet = plt.cm.get_cmap('jet', 256)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 29s, sys: 2.51 s, total: 1min 31s\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_splits = 6\n",
    "group_gap = 20\n",
    "fig, ax = plt.subplots()\n",
    "cv = PurgedGroupTimeSeriesSplit(n_splits = n_splits, group_gap = group_gap)\n",
    "plot_cv_indices(cv, X, y, train_data['date'].values, ax, n_splits, lw = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 11:07:42.325247: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-08 11:07:42.440590: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-08 11:07:46.110530: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_probability as tfp\n",
    "import os, gc\n",
    "import keras.backend as K\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(y_true, y_pred):\n",
    "    return tfp.stats.correlation(y_true, y_pred, sample_axis=0, event_axis=None, name='correlation')\n",
    "\n",
    "def mse_corr_loss(y_true, y_pred):\n",
    "    eta = corr(y_true, y_pred)\n",
    "    mse = tf.keras.losses.MeanSquaredError()\n",
    "    return mse(y_true, y_pred) + tf.constant(0.02)/tf.math.maximum(eta, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_ae_mlp(in_dim, out_dim, hidden_units, dropout_rates, lr = 2e-4):\n",
    "    \n",
    "    inp = keras.layers.Input(shape = (in_dim, ))\n",
    "    x0 = keras.layers.BatchNormalization()(inp)\n",
    "    \n",
    "    encoder = keras.layers.GaussianNoise(dropout_rates[0])(x0)\n",
    "    encoder = keras.layers.Dense(hidden_units[0])(encoder)\n",
    "    encoder = keras.layers.BatchNormalization()(encoder)\n",
    "    encoder = keras.layers.Activation('swish')(encoder)\n",
    "    \n",
    "    decoder = keras.layers.Dropout(dropout_rates[1])(encoder)\n",
    "    decoder = keras.layers.Dense(in_dim*2, activation='swish')(decoder)\n",
    "    decoder = keras.layers.Dense(in_dim, name = 'decoder')(decoder)\n",
    "    \n",
    "    x = keras.layers.Concatenate()([x0, encoder])\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dropout(dropout_rates[3])(x)\n",
    "    \n",
    "    for i in range(2, len(hidden_units)):\n",
    "        x = keras.layers.Dense(hidden_units[i])(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.Activation('swish')(x)\n",
    "        x = keras.layers.Dropout(dropout_rates[i + 2])(x)\n",
    "        \n",
    "    pred = keras.layers.Dense(out_dim, activation = 'gelu', name = 'pred')(x)\n",
    "    \n",
    "    model = keras.models.Model(inputs = inp, outputs = [decoder, pred])\n",
    "    model.compile(optimizer = keras.optimizers.Adam(learning_rate = lr),\n",
    "                  loss = {'decoder': keras.losses.MeanSquaredError(), \n",
    "                          'pred': mse_corr_loss, \n",
    "                         },\n",
    "                  metrics = {'decoder': keras.metrics.MeanAbsoluteError(name = 'MAE'), \n",
    "                             'pred': corr, \n",
    "                            }, \n",
    "                 )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'in_dim': X.shape[1], \n",
    "          'out_dim': 1, \n",
    "          'hidden_units': [96, 96, 896, 448, 448, 256], \n",
    "          'dropout_rates': [0.03527936123679956, 0.038424974585075086, 0.42409238408801436, 0.10431484318345882, 0.49230389137187497, 0.32024444956111164, 0.2716856145683449, 0.4379233941604448], \n",
    "          'lr':1e-3, \n",
    "         }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "59/59 [==============================] - 7s 46ms/step - loss: 15.8221 - decoder_loss: 0.6813 - pred_loss: 15.1408 - decoder_MAE: 0.4699 - pred_corr: 0.0306 - val_loss: 25.7687 - val_decoder_loss: 0.4138 - val_pred_loss: 25.3549 - val_decoder_MAE: 0.3758 - val_pred_corr: 0.0489\n",
      "Epoch 2/100\n",
      "59/59 [==============================] - 2s 37ms/step - loss: 2.8349 - decoder_loss: 0.3507 - pred_loss: 2.4842 - decoder_MAE: 0.3545 - pred_corr: 0.0528 - val_loss: 18.6517 - val_decoder_loss: 0.2687 - val_pred_loss: 18.3830 - val_decoder_MAE: 0.3173 - val_pred_corr: 0.0536\n",
      "Epoch 3/100\n",
      "59/59 [==============================] - 2s 37ms/step - loss: 2.2691 - decoder_loss: 0.2667 - pred_loss: 2.0024 - decoder_MAE: 0.3160 - pred_corr: 0.0613 - val_loss: 14.4127 - val_decoder_loss: 0.2170 - val_pred_loss: 14.1957 - val_decoder_MAE: 0.2897 - val_pred_corr: 0.0565\n",
      "Epoch 4/100\n",
      "59/59 [==============================] - 2s 36ms/step - loss: 1.9116 - decoder_loss: 0.2324 - pred_loss: 1.6792 - decoder_MAE: 0.2961 - pred_corr: 0.0692 - val_loss: 13.5530 - val_decoder_loss: 0.1902 - val_pred_loss: 13.3628 - val_decoder_MAE: 0.2727 - val_pred_corr: 0.0560\n",
      "Epoch 5/100\n",
      "59/59 [==============================] - 2s 37ms/step - loss: 1.7498 - decoder_loss: 0.2117 - pred_loss: 1.5381 - decoder_MAE: 0.2838 - pred_corr: 0.0740 - val_loss: 10.7181 - val_decoder_loss: 0.1748 - val_pred_loss: 10.5433 - val_decoder_MAE: 0.2618 - val_pred_corr: 0.0561\n",
      "Epoch 6/100\n",
      "59/59 [==============================] - 2s 36ms/step - loss: 1.7245 - decoder_loss: 0.2003 - pred_loss: 1.5242 - decoder_MAE: 0.2755 - pred_corr: 0.0767 - val_loss: 11.6089 - val_decoder_loss: 0.1632 - val_pred_loss: 11.4457 - val_decoder_MAE: 0.2530 - val_pred_corr: 0.0538\n",
      "Epoch 7/100\n",
      "59/59 [==============================] - 2s 37ms/step - loss: 1.5624 - decoder_loss: 0.1895 - pred_loss: 1.3729 - decoder_MAE: 0.2680 - pred_corr: 0.0816 - val_loss: 10.5424 - val_decoder_loss: 0.1540 - val_pred_loss: 10.3884 - val_decoder_MAE: 0.2460 - val_pred_corr: 0.0553\n",
      "Epoch 8/100\n",
      "59/59 [==============================] - 2s 38ms/step - loss: 1.4765 - decoder_loss: 0.1826 - pred_loss: 1.2939 - decoder_MAE: 0.2630 - pred_corr: 0.0865 - val_loss: 8.6008 - val_decoder_loss: 0.1491 - val_pred_loss: 8.4517 - val_decoder_MAE: 0.2415 - val_pred_corr: 0.0575\n",
      "Epoch 9/100\n",
      "59/59 [==============================] - 2s 37ms/step - loss: 1.4108 - decoder_loss: 0.1780 - pred_loss: 1.2328 - decoder_MAE: 0.2591 - pred_corr: 0.0898 - val_loss: 15.7778 - val_decoder_loss: 0.1422 - val_pred_loss: 15.6356 - val_decoder_MAE: 0.2363 - val_pred_corr: 0.0527\n",
      "Epoch 10/100\n",
      "59/59 [==============================] - 2s 37ms/step - loss: 1.3469 - decoder_loss: 0.1716 - pred_loss: 1.1752 - decoder_MAE: 0.2543 - pred_corr: 0.0923 - val_loss: 12.4407 - val_decoder_loss: 0.1360 - val_pred_loss: 12.3047 - val_decoder_MAE: 0.2311 - val_pred_corr: 0.0560\n",
      "Epoch 11/100\n",
      "59/59 [==============================] - 2s 37ms/step - loss: 1.2912 - decoder_loss: 0.1666 - pred_loss: 1.1246 - decoder_MAE: 0.2508 - pred_corr: 0.0964 - val_loss: 9.4806 - val_decoder_loss: 0.1337 - val_pred_loss: 9.3469 - val_decoder_MAE: 0.2284 - val_pred_corr: 0.0527\n",
      "Epoch 12/100\n",
      "59/59 [==============================] - 2s 37ms/step - loss: 1.2681 - decoder_loss: 0.1636 - pred_loss: 1.1045 - decoder_MAE: 0.2479 - pred_corr: 0.0982 - val_loss: 8.9703 - val_decoder_loss: 0.1299 - val_pred_loss: 8.8404 - val_decoder_MAE: 0.2252 - val_pred_corr: 0.0567\n",
      "Epoch 13/100\n",
      "59/59 [==============================] - 2s 37ms/step - loss: 1.2261 - decoder_loss: 0.1593 - pred_loss: 1.0667 - decoder_MAE: 0.2451 - pred_corr: 0.1015 - val_loss: 11.2577 - val_decoder_loss: 0.1270 - val_pred_loss: 11.1307 - val_decoder_MAE: 0.2218 - val_pred_corr: 0.0537\n",
      "Epoch 14/100\n",
      "59/59 [==============================] - 2s 37ms/step - loss: 1.1851 - decoder_loss: 0.1574 - pred_loss: 1.0276 - decoder_MAE: 0.2426 - pred_corr: 0.1044 - val_loss: 10.3355 - val_decoder_loss: 0.1235 - val_pred_loss: 10.2120 - val_decoder_MAE: 0.2191 - val_pred_corr: 0.0551\n",
      "Epoch 15/100\n",
      "59/59 [==============================] - 2s 37ms/step - loss: 1.1330 - decoder_loss: 0.1538 - pred_loss: 0.9792 - decoder_MAE: 0.2401 - pred_corr: 0.1086 - val_loss: 9.4458 - val_decoder_loss: 0.1205 - val_pred_loss: 9.3254 - val_decoder_MAE: 0.2164 - val_pred_corr: 0.0544\n",
      "Epoch 16/100\n",
      "59/59 [==============================] - 2s 36ms/step - loss: 1.1110 - decoder_loss: 0.1507 - pred_loss: 0.9603 - decoder_MAE: 0.2379 - pred_corr: 0.1109 - val_loss: 7.9675 - val_decoder_loss: 0.1188 - val_pred_loss: 7.8486 - val_decoder_MAE: 0.2140 - val_pred_corr: 0.0557\n",
      "Epoch 17/100\n",
      "59/59 [==============================] - 2s 36ms/step - loss: 1.1190 - decoder_loss: 0.1485 - pred_loss: 0.9705 - decoder_MAE: 0.2355 - pred_corr: 0.1096 - val_loss: 9.3883 - val_decoder_loss: 0.1162 - val_pred_loss: 9.2722 - val_decoder_MAE: 0.2121 - val_pred_corr: 0.0538\n",
      "Epoch 18/100\n",
      "59/59 [==============================] - 2s 36ms/step - loss: 1.0841 - decoder_loss: 0.1476 - pred_loss: 0.9365 - decoder_MAE: 0.2342 - pred_corr: 0.1133 - val_loss: 7.1836 - val_decoder_loss: 0.1142 - val_pred_loss: 7.0695 - val_decoder_MAE: 0.2101 - val_pred_corr: 0.0573\n",
      "Fold 0 val_correlation:\t 0.057468313723802567\n",
      "Epoch 1/100\n",
      "129/129 [==============================] - 8s 32ms/step - loss: 5.7636 - decoder_loss: 0.4866 - pred_loss: 5.2769 - decoder_MAE: 0.3996 - pred_corr: 0.0410 - val_loss: 30.1013 - val_decoder_loss: 0.2512 - val_pred_loss: 29.8501 - val_decoder_MAE: 0.3047 - val_pred_corr: 0.0529\n",
      "Epoch 2/100\n",
      "129/129 [==============================] - 3s 27ms/step - loss: 2.9578 - decoder_loss: 0.2374 - pred_loss: 2.7204 - decoder_MAE: 0.2977 - pred_corr: 0.0497 - val_loss: 27.1682 - val_decoder_loss: 0.1801 - val_pred_loss: 26.9881 - val_decoder_MAE: 0.2618 - val_pred_corr: 0.0524\n",
      "Epoch 3/100\n",
      "129/129 [==============================] - 4s 27ms/step - loss: 2.2659 - decoder_loss: 0.1959 - pred_loss: 2.0701 - decoder_MAE: 0.2709 - pred_corr: 0.0572 - val_loss: 17.2770 - val_decoder_loss: 0.1550 - val_pred_loss: 17.1219 - val_decoder_MAE: 0.2422 - val_pred_corr: 0.0509\n",
      "Epoch 4/100\n",
      "129/129 [==============================] - 4s 27ms/step - loss: 2.3908 - decoder_loss: 0.1800 - pred_loss: 2.2108 - decoder_MAE: 0.2590 - pred_corr: 0.0570 - val_loss: 21.9771 - val_decoder_loss: 0.1426 - val_pred_loss: 21.8346 - val_decoder_MAE: 0.2321 - val_pred_corr: 0.0540\n",
      "Epoch 5/100\n",
      "129/129 [==============================] - 4s 27ms/step - loss: 1.9790 - decoder_loss: 0.1687 - pred_loss: 1.8103 - decoder_MAE: 0.2509 - pred_corr: 0.0664 - val_loss: 21.1602 - val_decoder_loss: 0.1337 - val_pred_loss: 21.0265 - val_decoder_MAE: 0.2249 - val_pred_corr: 0.0524\n",
      "Epoch 6/100\n",
      "129/129 [==============================] - 4s 27ms/step - loss: 1.6937 - decoder_loss: 0.1606 - pred_loss: 1.5330 - decoder_MAE: 0.2448 - pred_corr: 0.0730 - val_loss: 16.7038 - val_decoder_loss: 0.1271 - val_pred_loss: 16.5767 - val_decoder_MAE: 0.2185 - val_pred_corr: 0.0587\n",
      "Epoch 7/100\n",
      "129/129 [==============================] - 4s 27ms/step - loss: 1.5986 - decoder_loss: 0.1545 - pred_loss: 1.4441 - decoder_MAE: 0.2402 - pred_corr: 0.0768 - val_loss: 18.8841 - val_decoder_loss: 0.1220 - val_pred_loss: 18.7621 - val_decoder_MAE: 0.2145 - val_pred_corr: 0.0640\n",
      "Epoch 8/100\n",
      "129/129 [==============================] - 4s 28ms/step - loss: 1.7438 - decoder_loss: 0.1517 - pred_loss: 1.5921 - decoder_MAE: 0.2374 - pred_corr: 0.0764 - val_loss: 22.9746 - val_decoder_loss: 0.1226 - val_pred_loss: 22.8520 - val_decoder_MAE: 0.2145 - val_pred_corr: 0.0571\n",
      "Epoch 9/100\n",
      "129/129 [==============================] - 4s 27ms/step - loss: 1.7051 - decoder_loss: 0.1508 - pred_loss: 1.5543 - decoder_MAE: 0.2365 - pred_corr: 0.0753 - val_loss: 16.9232 - val_decoder_loss: 0.1180 - val_pred_loss: 16.8052 - val_decoder_MAE: 0.2098 - val_pred_corr: 0.0643\n",
      "Epoch 10/100\n",
      "129/129 [==============================] - 4s 28ms/step - loss: 1.4739 - decoder_loss: 0.1461 - pred_loss: 1.3279 - decoder_MAE: 0.2326 - pred_corr: 0.0820 - val_loss: 17.0623 - val_decoder_loss: 0.1138 - val_pred_loss: 16.9485 - val_decoder_MAE: 0.2063 - val_pred_corr: 0.0654\n",
      "Epoch 11/100\n",
      "129/129 [==============================] - 4s 28ms/step - loss: 1.4163 - decoder_loss: 0.1436 - pred_loss: 1.2726 - decoder_MAE: 0.2303 - pred_corr: 0.0861 - val_loss: 15.8053 - val_decoder_loss: 0.1121 - val_pred_loss: 15.6932 - val_decoder_MAE: 0.2038 - val_pred_corr: 0.0620\n",
      "Epoch 12/100\n",
      "129/129 [==============================] - 4s 27ms/step - loss: 1.3353 - decoder_loss: 0.1413 - pred_loss: 1.1940 - decoder_MAE: 0.2278 - pred_corr: 0.0900 - val_loss: 15.9952 - val_decoder_loss: 0.1089 - val_pred_loss: 15.8863 - val_decoder_MAE: 0.2020 - val_pred_corr: 0.0630\n",
      "Epoch 13/100\n",
      "129/129 [==============================] - 4s 27ms/step - loss: 1.2877 - decoder_loss: 0.1387 - pred_loss: 1.1490 - decoder_MAE: 0.2258 - pred_corr: 0.0939 - val_loss: 16.0461 - val_decoder_loss: 0.1067 - val_pred_loss: 15.9394 - val_decoder_MAE: 0.1989 - val_pred_corr: 0.0648\n",
      "Epoch 14/100\n",
      "129/129 [==============================] - 4s 27ms/step - loss: 1.2426 - decoder_loss: 0.1368 - pred_loss: 1.1058 - decoder_MAE: 0.2240 - pred_corr: 0.0965 - val_loss: 16.2767 - val_decoder_loss: 0.1052 - val_pred_loss: 16.1715 - val_decoder_MAE: 0.1978 - val_pred_corr: 0.0649\n",
      "Epoch 15/100\n",
      "129/129 [==============================] - 3s 27ms/step - loss: 1.2290 - decoder_loss: 0.1346 - pred_loss: 1.0944 - decoder_MAE: 0.2220 - pred_corr: 0.0980 - val_loss: 16.3416 - val_decoder_loss: 0.1039 - val_pred_loss: 16.2378 - val_decoder_MAE: 0.1956 - val_pred_corr: 0.0647\n",
      "Epoch 16/100\n",
      "129/129 [==============================] - 3s 27ms/step - loss: 1.1848 - decoder_loss: 0.1325 - pred_loss: 1.0523 - decoder_MAE: 0.2204 - pred_corr: 0.1013 - val_loss: 15.4090 - val_decoder_loss: 0.1019 - val_pred_loss: 15.3071 - val_decoder_MAE: 0.1942 - val_pred_corr: 0.0627\n",
      "Epoch 17/100\n",
      "129/129 [==============================] - 4s 27ms/step - loss: 1.1639 - decoder_loss: 0.1306 - pred_loss: 1.0333 - decoder_MAE: 0.2184 - pred_corr: 0.1017 - val_loss: 16.2827 - val_decoder_loss: 0.0995 - val_pred_loss: 16.1831 - val_decoder_MAE: 0.1919 - val_pred_corr: 0.0650\n",
      "Epoch 18/100\n",
      "129/129 [==============================] - 4s 27ms/step - loss: 1.1342 - decoder_loss: 0.1295 - pred_loss: 1.0047 - decoder_MAE: 0.2171 - pred_corr: 0.1043 - val_loss: 15.4416 - val_decoder_loss: 0.0979 - val_pred_loss: 15.3437 - val_decoder_MAE: 0.1894 - val_pred_corr: 0.0631\n",
      "Epoch 19/100\n",
      "129/129 [==============================] - 3s 27ms/step - loss: 1.1169 - decoder_loss: 0.1276 - pred_loss: 0.9893 - decoder_MAE: 0.2153 - pred_corr: 0.1056 - val_loss: 16.5432 - val_decoder_loss: 0.0969 - val_pred_loss: 16.4463 - val_decoder_MAE: 0.1888 - val_pred_corr: 0.0649\n",
      "Epoch 20/100\n",
      "129/129 [==============================] - 4s 28ms/step - loss: 1.0910 - decoder_loss: 0.1261 - pred_loss: 0.9649 - decoder_MAE: 0.2141 - pred_corr: 0.1085 - val_loss: 15.0693 - val_decoder_loss: 0.0968 - val_pred_loss: 14.9724 - val_decoder_MAE: 0.1879 - val_pred_corr: 0.0655\n",
      "Fold 1 val_correlation:\t 0.0654989629983902\n",
      "Epoch 1/100\n",
      "205/205 [==============================] - 10s 27ms/step - loss: 4.3185 - decoder_loss: 0.4072 - pred_loss: 3.9112 - decoder_MAE: 0.3659 - pred_corr: 0.0457 - val_loss: 32.0618 - val_decoder_loss: 0.2008 - val_pred_loss: 31.8609 - val_decoder_MAE: 0.2796 - val_pred_corr: 0.0299\n",
      "Epoch 2/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 2.8149 - decoder_loss: 0.2054 - pred_loss: 2.6095 - decoder_MAE: 0.2764 - pred_corr: 0.0531 - val_loss: 15.0944 - val_decoder_loss: 0.1501 - val_pred_loss: 14.9442 - val_decoder_MAE: 0.2437 - val_pred_corr: 0.0507\n",
      "Epoch 3/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 2.1511 - decoder_loss: 0.1762 - pred_loss: 1.9749 - decoder_MAE: 0.2556 - pred_corr: 0.0610 - val_loss: 17.2476 - val_decoder_loss: 0.1324 - val_pred_loss: 17.1153 - val_decoder_MAE: 0.2288 - val_pred_corr: 0.0494\n",
      "Epoch 4/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 2.1390 - decoder_loss: 0.1641 - pred_loss: 1.9749 - decoder_MAE: 0.2457 - pred_corr: 0.0615 - val_loss: 12.3316 - val_decoder_loss: 0.1222 - val_pred_loss: 12.2094 - val_decoder_MAE: 0.2191 - val_pred_corr: 0.0493\n",
      "Epoch 5/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 1.8073 - decoder_loss: 0.1557 - pred_loss: 1.6516 - decoder_MAE: 0.2388 - pred_corr: 0.0694 - val_loss: 10.7191 - val_decoder_loss: 0.1149 - val_pred_loss: 10.6042 - val_decoder_MAE: 0.2130 - val_pred_corr: 0.0550\n",
      "Epoch 6/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 1.6276 - decoder_loss: 0.1485 - pred_loss: 1.4791 - decoder_MAE: 0.2334 - pred_corr: 0.0754 - val_loss: 10.4641 - val_decoder_loss: 0.1096 - val_pred_loss: 10.3545 - val_decoder_MAE: 0.2076 - val_pred_corr: 0.0561\n",
      "Epoch 7/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 1.5408 - decoder_loss: 0.1442 - pred_loss: 1.3966 - decoder_MAE: 0.2293 - pred_corr: 0.0787 - val_loss: 10.5407 - val_decoder_loss: 0.1060 - val_pred_loss: 10.4347 - val_decoder_MAE: 0.2048 - val_pred_corr: 0.0543\n",
      "Epoch 8/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 1.4243 - decoder_loss: 0.1396 - pred_loss: 1.2847 - decoder_MAE: 0.2254 - pred_corr: 0.0841 - val_loss: 8.4029 - val_decoder_loss: 0.1026 - val_pred_loss: 8.3002 - val_decoder_MAE: 0.2005 - val_pred_corr: 0.0553\n",
      "Epoch 9/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 1.4020 - decoder_loss: 0.1359 - pred_loss: 1.2661 - decoder_MAE: 0.2223 - pred_corr: 0.0859 - val_loss: 9.3997 - val_decoder_loss: 0.0989 - val_pred_loss: 9.3008 - val_decoder_MAE: 0.1965 - val_pred_corr: 0.0554\n",
      "Epoch 10/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 1.3369 - decoder_loss: 0.1329 - pred_loss: 1.2040 - decoder_MAE: 0.2197 - pred_corr: 0.0897 - val_loss: 9.3089 - val_decoder_loss: 0.0954 - val_pred_loss: 9.2135 - val_decoder_MAE: 0.1931 - val_pred_corr: 0.0564\n",
      "Epoch 11/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 1.4268 - decoder_loss: 0.1306 - pred_loss: 1.2962 - decoder_MAE: 0.2174 - pred_corr: 0.0837 - val_loss: 9.3359 - val_decoder_loss: 0.0938 - val_pred_loss: 9.2422 - val_decoder_MAE: 0.1911 - val_pred_corr: 0.0569\n",
      "Epoch 12/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 1.2802 - decoder_loss: 0.1278 - pred_loss: 1.1524 - decoder_MAE: 0.2148 - pred_corr: 0.0924 - val_loss: 17.3712 - val_decoder_loss: 0.0941 - val_pred_loss: 17.2771 - val_decoder_MAE: 0.1897 - val_pred_corr: 0.0441\n",
      "Epoch 13/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 2.3493 - decoder_loss: 0.1394 - pred_loss: 2.2099 - decoder_MAE: 0.2228 - pred_corr: 0.0611 - val_loss: 13.1025 - val_decoder_loss: 0.0974 - val_pred_loss: 13.0051 - val_decoder_MAE: 0.1934 - val_pred_corr: 0.0516\n",
      "Epoch 14/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 1.5927 - decoder_loss: 0.1317 - pred_loss: 1.4609 - decoder_MAE: 0.2167 - pred_corr: 0.0773 - val_loss: 10.5285 - val_decoder_loss: 0.0933 - val_pred_loss: 10.4352 - val_decoder_MAE: 0.1894 - val_pred_corr: 0.0568\n",
      "Epoch 15/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 1.4356 - decoder_loss: 0.1285 - pred_loss: 1.3071 - decoder_MAE: 0.2135 - pred_corr: 0.0833 - val_loss: 9.4084 - val_decoder_loss: 0.0917 - val_pred_loss: 9.3167 - val_decoder_MAE: 0.1867 - val_pred_corr: 0.0570\n",
      "Epoch 16/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 1.3499 - decoder_loss: 0.1262 - pred_loss: 1.2237 - decoder_MAE: 0.2112 - pred_corr: 0.0884 - val_loss: 9.0026 - val_decoder_loss: 0.0887 - val_pred_loss: 8.9140 - val_decoder_MAE: 0.1843 - val_pred_corr: 0.0608\n",
      "Epoch 17/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 1.3078 - decoder_loss: 0.1240 - pred_loss: 1.1838 - decoder_MAE: 0.2094 - pred_corr: 0.0909 - val_loss: 9.3992 - val_decoder_loss: 0.0878 - val_pred_loss: 9.3114 - val_decoder_MAE: 0.1821 - val_pred_corr: 0.0587\n",
      "Epoch 18/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 1.2373 - decoder_loss: 0.1217 - pred_loss: 1.1156 - decoder_MAE: 0.2072 - pred_corr: 0.0953 - val_loss: 9.5336 - val_decoder_loss: 0.0855 - val_pred_loss: 9.4481 - val_decoder_MAE: 0.1801 - val_pred_corr: 0.0608\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 5s 25ms/step - loss: 1.2100 - decoder_loss: 0.1199 - pred_loss: 1.0901 - decoder_MAE: 0.2053 - pred_corr: 0.0969 - val_loss: 9.3301 - val_decoder_loss: 0.0851 - val_pred_loss: 9.2451 - val_decoder_MAE: 0.1785 - val_pred_corr: 0.0631\n",
      "Epoch 20/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 1.1738 - decoder_loss: 0.1191 - pred_loss: 1.0547 - decoder_MAE: 0.2040 - pred_corr: 0.0997 - val_loss: 8.9483 - val_decoder_loss: 0.0825 - val_pred_loss: 8.8657 - val_decoder_MAE: 0.1764 - val_pred_corr: 0.0656\n",
      "Epoch 21/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 1.1412 - decoder_loss: 0.1167 - pred_loss: 1.0244 - decoder_MAE: 0.2019 - pred_corr: 0.1025 - val_loss: 8.9358 - val_decoder_loss: 0.0808 - val_pred_loss: 8.8550 - val_decoder_MAE: 0.1754 - val_pred_corr: 0.0651\n",
      "Epoch 22/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 1.1115 - decoder_loss: 0.1143 - pred_loss: 0.9972 - decoder_MAE: 0.2000 - pred_corr: 0.1049 - val_loss: 8.8817 - val_decoder_loss: 0.0803 - val_pred_loss: 8.8014 - val_decoder_MAE: 0.1754 - val_pred_corr: 0.0672\n",
      "Epoch 23/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 1.0935 - decoder_loss: 0.1131 - pred_loss: 0.9805 - decoder_MAE: 0.1987 - pred_corr: 0.1065 - val_loss: 8.3575 - val_decoder_loss: 0.0774 - val_pred_loss: 8.2801 - val_decoder_MAE: 0.1706 - val_pred_corr: 0.0678\n",
      "Epoch 24/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 1.0759 - decoder_loss: 0.1120 - pred_loss: 0.9639 - decoder_MAE: 0.1971 - pred_corr: 0.1081 - val_loss: 7.6161 - val_decoder_loss: 0.0760 - val_pred_loss: 7.5402 - val_decoder_MAE: 0.1695 - val_pred_corr: 0.0698\n",
      "Epoch 25/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 1.0487 - decoder_loss: 0.1099 - pred_loss: 0.9389 - decoder_MAE: 0.1951 - pred_corr: 0.1107 - val_loss: 7.2176 - val_decoder_loss: 0.0753 - val_pred_loss: 7.1423 - val_decoder_MAE: 0.1679 - val_pred_corr: 0.0641\n",
      "Epoch 26/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 1.0269 - decoder_loss: 0.1084 - pred_loss: 0.9185 - decoder_MAE: 0.1937 - pred_corr: 0.1135 - val_loss: 9.6849 - val_decoder_loss: 0.0752 - val_pred_loss: 9.6097 - val_decoder_MAE: 0.1667 - val_pred_corr: 0.0589\n",
      "Epoch 27/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 1.3116 - decoder_loss: 0.1131 - pred_loss: 1.1985 - decoder_MAE: 0.1978 - pred_corr: 0.0912 - val_loss: 7.5762 - val_decoder_loss: 0.0761 - val_pred_loss: 7.5000 - val_decoder_MAE: 0.1698 - val_pred_corr: 0.0661\n",
      "Epoch 28/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 1.0762 - decoder_loss: 0.1090 - pred_loss: 0.9671 - decoder_MAE: 0.1939 - pred_corr: 0.1080 - val_loss: 7.0243 - val_decoder_loss: 0.0738 - val_pred_loss: 6.9505 - val_decoder_MAE: 0.1655 - val_pred_corr: 0.0667\n",
      "Epoch 29/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 1.0369 - decoder_loss: 0.1068 - pred_loss: 0.9301 - decoder_MAE: 0.1917 - pred_corr: 0.1122 - val_loss: 7.6715 - val_decoder_loss: 0.0715 - val_pred_loss: 7.6000 - val_decoder_MAE: 0.1638 - val_pred_corr: 0.0685\n",
      "Epoch 30/100\n",
      "205/205 [==============================] - 5s 25ms/step - loss: 0.9837 - decoder_loss: 0.1055 - pred_loss: 0.8782 - decoder_MAE: 0.1900 - pred_corr: 0.1180 - val_loss: 7.8094 - val_decoder_loss: 0.0706 - val_pred_loss: 7.7388 - val_decoder_MAE: 0.1619 - val_pred_corr: 0.0688\n",
      "Epoch 31/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 0.9753 - decoder_loss: 0.1036 - pred_loss: 0.8716 - decoder_MAE: 0.1883 - pred_corr: 0.1194 - val_loss: 7.8925 - val_decoder_loss: 0.0695 - val_pred_loss: 7.8231 - val_decoder_MAE: 0.1614 - val_pred_corr: 0.0704\n",
      "Epoch 32/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 0.9346 - decoder_loss: 0.1022 - pred_loss: 0.8324 - decoder_MAE: 0.1865 - pred_corr: 0.1248 - val_loss: 7.2440 - val_decoder_loss: 0.0676 - val_pred_loss: 7.1764 - val_decoder_MAE: 0.1578 - val_pred_corr: 0.0678\n",
      "Epoch 33/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 0.9230 - decoder_loss: 0.1002 - pred_loss: 0.8227 - decoder_MAE: 0.1850 - pred_corr: 0.1262 - val_loss: 7.6041 - val_decoder_loss: 0.0655 - val_pred_loss: 7.5386 - val_decoder_MAE: 0.1562 - val_pred_corr: 0.0687\n",
      "Epoch 34/100\n",
      "205/205 [==============================] - 5s 23ms/step - loss: 0.9023 - decoder_loss: 0.1002 - pred_loss: 0.8021 - decoder_MAE: 0.1837 - pred_corr: 0.1285 - val_loss: 7.8202 - val_decoder_loss: 0.0652 - val_pred_loss: 7.7550 - val_decoder_MAE: 0.1552 - val_pred_corr: 0.0698\n",
      "Epoch 35/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 0.8871 - decoder_loss: 0.0985 - pred_loss: 0.7887 - decoder_MAE: 0.1821 - pred_corr: 0.1312 - val_loss: 9.0845 - val_decoder_loss: 0.0635 - val_pred_loss: 9.0210 - val_decoder_MAE: 0.1527 - val_pred_corr: 0.0690\n",
      "Epoch 36/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 0.8669 - decoder_loss: 0.0966 - pred_loss: 0.7703 - decoder_MAE: 0.1804 - pred_corr: 0.1341 - val_loss: 9.7116 - val_decoder_loss: 0.0623 - val_pred_loss: 9.6493 - val_decoder_MAE: 0.1513 - val_pred_corr: 0.0665\n",
      "Epoch 37/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 0.8521 - decoder_loss: 0.0954 - pred_loss: 0.7567 - decoder_MAE: 0.1790 - pred_corr: 0.1367 - val_loss: 7.4933 - val_decoder_loss: 0.0606 - val_pred_loss: 7.4327 - val_decoder_MAE: 0.1493 - val_pred_corr: 0.0734\n",
      "Epoch 38/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 0.8347 - decoder_loss: 0.0936 - pred_loss: 0.7411 - decoder_MAE: 0.1774 - pred_corr: 0.1388 - val_loss: 8.0693 - val_decoder_loss: 0.0595 - val_pred_loss: 8.0098 - val_decoder_MAE: 0.1476 - val_pred_corr: 0.0717\n",
      "Epoch 39/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 0.8177 - decoder_loss: 0.0921 - pred_loss: 0.7256 - decoder_MAE: 0.1756 - pred_corr: 0.1414 - val_loss: 9.9148 - val_decoder_loss: 0.0589 - val_pred_loss: 9.8559 - val_decoder_MAE: 0.1460 - val_pred_corr: 0.0715\n",
      "Epoch 40/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 0.7964 - decoder_loss: 0.0909 - pred_loss: 0.7055 - decoder_MAE: 0.1743 - pred_corr: 0.1455 - val_loss: 7.4365 - val_decoder_loss: 0.0575 - val_pred_loss: 7.3790 - val_decoder_MAE: 0.1451 - val_pred_corr: 0.0731\n",
      "Epoch 41/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 0.7866 - decoder_loss: 0.0899 - pred_loss: 0.6967 - decoder_MAE: 0.1732 - pred_corr: 0.1469 - val_loss: 8.7512 - val_decoder_loss: 0.0567 - val_pred_loss: 8.6945 - val_decoder_MAE: 0.1431 - val_pred_corr: 0.0720\n",
      "Epoch 42/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 0.7781 - decoder_loss: 0.0888 - pred_loss: 0.6894 - decoder_MAE: 0.1718 - pred_corr: 0.1485 - val_loss: 7.4486 - val_decoder_loss: 0.0554 - val_pred_loss: 7.3932 - val_decoder_MAE: 0.1418 - val_pred_corr: 0.0745\n",
      "Epoch 43/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 0.7614 - decoder_loss: 0.0877 - pred_loss: 0.6738 - decoder_MAE: 0.1706 - pred_corr: 0.1519 - val_loss: 7.9881 - val_decoder_loss: 0.0547 - val_pred_loss: 7.9334 - val_decoder_MAE: 0.1405 - val_pred_corr: 0.0715\n",
      "Epoch 44/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 0.7487 - decoder_loss: 0.0865 - pred_loss: 0.6622 - decoder_MAE: 0.1693 - pred_corr: 0.1543 - val_loss: 10.9250 - val_decoder_loss: 0.0546 - val_pred_loss: 10.8704 - val_decoder_MAE: 0.1400 - val_pred_corr: 0.0691\n",
      "Epoch 45/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 0.9020 - decoder_loss: 0.0916 - pred_loss: 0.8104 - decoder_MAE: 0.1738 - pred_corr: 0.1291 - val_loss: 9.1306 - val_decoder_loss: 0.0558 - val_pred_loss: 9.0748 - val_decoder_MAE: 0.1417 - val_pred_corr: 0.0762\n",
      "Epoch 46/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 0.8170 - decoder_loss: 0.0890 - pred_loss: 0.7280 - decoder_MAE: 0.1710 - pred_corr: 0.1423 - val_loss: 7.3186 - val_decoder_loss: 0.0546 - val_pred_loss: 7.2640 - val_decoder_MAE: 0.1396 - val_pred_corr: 0.0751\n",
      "Epoch 47/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 0.7544 - decoder_loss: 0.0854 - pred_loss: 0.6690 - decoder_MAE: 0.1680 - pred_corr: 0.1534 - val_loss: 8.4955 - val_decoder_loss: 0.0525 - val_pred_loss: 8.4430 - val_decoder_MAE: 0.1377 - val_pred_corr: 0.0739\n",
      "Epoch 48/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 0.7325 - decoder_loss: 0.0848 - pred_loss: 0.6477 - decoder_MAE: 0.1666 - pred_corr: 0.1575 - val_loss: 7.8068 - val_decoder_loss: 0.0513 - val_pred_loss: 7.7556 - val_decoder_MAE: 0.1357 - val_pred_corr: 0.0750\n",
      "Epoch 49/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 0.7231 - decoder_loss: 0.0835 - pred_loss: 0.6395 - decoder_MAE: 0.1655 - pred_corr: 0.1593 - val_loss: 7.5982 - val_decoder_loss: 0.0500 - val_pred_loss: 7.5482 - val_decoder_MAE: 0.1331 - val_pred_corr: 0.0762\n",
      "Epoch 50/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 0.7076 - decoder_loss: 0.0827 - pred_loss: 0.6249 - decoder_MAE: 0.1642 - pred_corr: 0.1630 - val_loss: 7.5285 - val_decoder_loss: 0.0496 - val_pred_loss: 7.4789 - val_decoder_MAE: 0.1330 - val_pred_corr: 0.0749\n",
      "Epoch 51/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 0.7024 - decoder_loss: 0.0811 - pred_loss: 0.6214 - decoder_MAE: 0.1630 - pred_corr: 0.1638 - val_loss: 8.3093 - val_decoder_loss: 0.0480 - val_pred_loss: 8.2612 - val_decoder_MAE: 0.1308 - val_pred_corr: 0.0767\n",
      "Epoch 52/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 0.6901 - decoder_loss: 0.0806 - pred_loss: 0.6095 - decoder_MAE: 0.1623 - pred_corr: 0.1671 - val_loss: 7.6011 - val_decoder_loss: 0.0482 - val_pred_loss: 7.5529 - val_decoder_MAE: 0.1304 - val_pred_corr: 0.0772\n",
      "Epoch 53/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 0.6740 - decoder_loss: 0.0791 - pred_loss: 0.5949 - decoder_MAE: 0.1606 - pred_corr: 0.1712 - val_loss: 7.2904 - val_decoder_loss: 0.0466 - val_pred_loss: 7.2439 - val_decoder_MAE: 0.1285 - val_pred_corr: 0.0785\n",
      "Epoch 54/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 0.6628 - decoder_loss: 0.0787 - pred_loss: 0.5841 - decoder_MAE: 0.1598 - pred_corr: 0.1736 - val_loss: 7.7044 - val_decoder_loss: 0.0464 - val_pred_loss: 7.6580 - val_decoder_MAE: 0.1288 - val_pred_corr: 0.0790\n",
      "Epoch 55/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 0.6623 - decoder_loss: 0.0786 - pred_loss: 0.5837 - decoder_MAE: 0.1594 - pred_corr: 0.1742 - val_loss: 7.2603 - val_decoder_loss: 0.0453 - val_pred_loss: 7.2150 - val_decoder_MAE: 0.1277 - val_pred_corr: 0.0843\n",
      "Epoch 56/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 0.6605 - decoder_loss: 0.0772 - pred_loss: 0.5833 - decoder_MAE: 0.1580 - pred_corr: 0.1748 - val_loss: 8.5893 - val_decoder_loss: 0.0454 - val_pred_loss: 8.5439 - val_decoder_MAE: 0.1253 - val_pred_corr: 0.0755\n",
      "Epoch 57/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 0.6819 - decoder_loss: 0.0769 - pred_loss: 0.6049 - decoder_MAE: 0.1582 - pred_corr: 0.1697 - val_loss: 7.2194 - val_decoder_loss: 0.0444 - val_pred_loss: 7.1750 - val_decoder_MAE: 0.1250 - val_pred_corr: 0.0783\n",
      "Epoch 58/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 0.6420 - decoder_loss: 0.0760 - pred_loss: 0.5660 - decoder_MAE: 0.1566 - pred_corr: 0.1792 - val_loss: 10.5932 - val_decoder_loss: 0.0444 - val_pred_loss: 10.5488 - val_decoder_MAE: 0.1264 - val_pred_corr: 0.0815\n",
      "Epoch 59/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 0.6345 - decoder_loss: 0.0754 - pred_loss: 0.5592 - decoder_MAE: 0.1554 - pred_corr: 0.1815 - val_loss: 7.8777 - val_decoder_loss: 0.0429 - val_pred_loss: 7.8347 - val_decoder_MAE: 0.1226 - val_pred_corr: 0.0833\n",
      "Epoch 60/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 0.6265 - decoder_loss: 0.0742 - pred_loss: 0.5523 - decoder_MAE: 0.1546 - pred_corr: 0.1846 - val_loss: 8.9949 - val_decoder_loss: 0.0427 - val_pred_loss: 8.9522 - val_decoder_MAE: 0.1225 - val_pred_corr: 0.0829\n",
      "Epoch 61/100\n",
      "205/205 [==============================] - 5s 23ms/step - loss: 0.6095 - decoder_loss: 0.0737 - pred_loss: 0.5358 - decoder_MAE: 0.1534 - pred_corr: 0.1897 - val_loss: 13.6456 - val_decoder_loss: 0.0418 - val_pred_loss: 13.6038 - val_decoder_MAE: 0.1201 - val_pred_corr: 0.0743\n",
      "Epoch 62/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 0.6034 - decoder_loss: 0.0726 - pred_loss: 0.5308 - decoder_MAE: 0.1524 - pred_corr: 0.1911 - val_loss: 10.6493 - val_decoder_loss: 0.0411 - val_pred_loss: 10.6082 - val_decoder_MAE: 0.1189 - val_pred_corr: 0.0831\n",
      "Epoch 63/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 0.5957 - decoder_loss: 0.0716 - pred_loss: 0.5241 - decoder_MAE: 0.1513 - pred_corr: 0.1940 - val_loss: 10.0554 - val_decoder_loss: 0.0413 - val_pred_loss: 10.0142 - val_decoder_MAE: 0.1207 - val_pred_corr: 0.0792\n",
      "Epoch 64/100\n",
      "205/205 [==============================] - 5s 23ms/step - loss: 0.5920 - decoder_loss: 0.0709 - pred_loss: 0.5211 - decoder_MAE: 0.1508 - pred_corr: 0.1952 - val_loss: 11.1571 - val_decoder_loss: 0.0400 - val_pred_loss: 11.1171 - val_decoder_MAE: 0.1180 - val_pred_corr: 0.0758\n",
      "Epoch 65/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 0.5854 - decoder_loss: 0.0705 - pred_loss: 0.5148 - decoder_MAE: 0.1500 - pred_corr: 0.1966 - val_loss: 10.9574 - val_decoder_loss: 0.0390 - val_pred_loss: 10.9183 - val_decoder_MAE: 0.1155 - val_pred_corr: 0.0726\n",
      "Fold 2 val_correlation:\t 0.08434924483299255\n",
      "Epoch 1/100\n",
      "280/280 [==============================] - 11s 25ms/step - loss: 3.0967 - decoder_loss: 0.3464 - pred_loss: 2.7503 - decoder_MAE: 0.3387 - pred_corr: 0.0539 - val_loss: 28.2674 - val_decoder_loss: 0.1763 - val_pred_loss: 28.0911 - val_decoder_MAE: 0.2611 - val_pred_corr: 0.0448\n",
      "Epoch 2/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 2.5220 - decoder_loss: 0.1913 - pred_loss: 2.3308 - decoder_MAE: 0.2646 - pred_corr: 0.0589 - val_loss: 29.7295 - val_decoder_loss: 0.1434 - val_pred_loss: 29.5861 - val_decoder_MAE: 0.2344 - val_pred_corr: 0.0405\n",
      "Epoch 3/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 1.9471 - decoder_loss: 0.1673 - pred_loss: 1.7798 - decoder_MAE: 0.2470 - pred_corr: 0.0656 - val_loss: 31.6327 - val_decoder_loss: 0.1275 - val_pred_loss: 31.5052 - val_decoder_MAE: 0.2205 - val_pred_corr: 0.0413\n",
      "Epoch 4/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 1.7180 - decoder_loss: 0.1562 - pred_loss: 1.5618 - decoder_MAE: 0.2378 - pred_corr: 0.0718 - val_loss: 29.0958 - val_decoder_loss: 0.1191 - val_pred_loss: 28.9767 - val_decoder_MAE: 0.2123 - val_pred_corr: 0.0426\n",
      "Epoch 5/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 1.5974 - decoder_loss: 0.1482 - pred_loss: 1.4493 - decoder_MAE: 0.2314 - pred_corr: 0.0769 - val_loss: 29.5725 - val_decoder_loss: 0.1134 - val_pred_loss: 29.4591 - val_decoder_MAE: 0.2079 - val_pred_corr: 0.0433\n",
      "Epoch 6/100\n",
      "280/280 [==============================] - 7s 23ms/step - loss: 1.5058 - decoder_loss: 0.1428 - pred_loss: 1.3630 - decoder_MAE: 0.2268 - pred_corr: 0.0805 - val_loss: 29.2133 - val_decoder_loss: 0.1091 - val_pred_loss: 29.1042 - val_decoder_MAE: 0.2032 - val_pred_corr: 0.0454\n",
      "Epoch 7/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 1.4198 - decoder_loss: 0.1391 - pred_loss: 1.2807 - decoder_MAE: 0.2231 - pred_corr: 0.0844 - val_loss: 29.5786 - val_decoder_loss: 0.1044 - val_pred_loss: 29.4741 - val_decoder_MAE: 0.1982 - val_pred_corr: 0.0453\n",
      "Epoch 8/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 1.3796 - decoder_loss: 0.1346 - pred_loss: 1.2449 - decoder_MAE: 0.2192 - pred_corr: 0.0870 - val_loss: 30.5559 - val_decoder_loss: 0.1023 - val_pred_loss: 30.4536 - val_decoder_MAE: 0.1952 - val_pred_corr: 0.0446\n",
      "Epoch 9/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 1.2994 - decoder_loss: 0.1311 - pred_loss: 1.1683 - decoder_MAE: 0.2162 - pred_corr: 0.0917 - val_loss: 29.7554 - val_decoder_loss: 0.0993 - val_pred_loss: 29.6561 - val_decoder_MAE: 0.1924 - val_pred_corr: 0.0459\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280/280 [==============================] - 6s 23ms/step - loss: 1.2380 - decoder_loss: 0.1289 - pred_loss: 1.1091 - decoder_MAE: 0.2136 - pred_corr: 0.0953 - val_loss: 27.8373 - val_decoder_loss: 0.0959 - val_pred_loss: 27.7414 - val_decoder_MAE: 0.1884 - val_pred_corr: 0.0476\n",
      "Epoch 11/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 1.1993 - decoder_loss: 0.1253 - pred_loss: 1.0740 - decoder_MAE: 0.2105 - pred_corr: 0.0981 - val_loss: 25.5061 - val_decoder_loss: 0.0920 - val_pred_loss: 25.4142 - val_decoder_MAE: 0.1848 - val_pred_corr: 0.0505\n",
      "Epoch 12/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 1.1829 - decoder_loss: 0.1226 - pred_loss: 1.0603 - decoder_MAE: 0.2080 - pred_corr: 0.0998 - val_loss: 29.6474 - val_decoder_loss: 0.0906 - val_pred_loss: 29.5567 - val_decoder_MAE: 0.1833 - val_pred_corr: 0.0487\n",
      "Epoch 13/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 1.1460 - decoder_loss: 0.1206 - pred_loss: 1.0254 - decoder_MAE: 0.2059 - pred_corr: 0.1026 - val_loss: 30.1888 - val_decoder_loss: 0.0874 - val_pred_loss: 30.1013 - val_decoder_MAE: 0.1803 - val_pred_corr: 0.0484\n",
      "Epoch 14/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 1.1126 - decoder_loss: 0.1178 - pred_loss: 0.9948 - decoder_MAE: 0.2031 - pred_corr: 0.1060 - val_loss: 30.9612 - val_decoder_loss: 0.0858 - val_pred_loss: 30.8754 - val_decoder_MAE: 0.1775 - val_pred_corr: 0.0507\n",
      "Epoch 15/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 1.0659 - decoder_loss: 0.1153 - pred_loss: 0.9507 - decoder_MAE: 0.2007 - pred_corr: 0.1100 - val_loss: 27.1926 - val_decoder_loss: 0.0854 - val_pred_loss: 27.1071 - val_decoder_MAE: 0.1757 - val_pred_corr: 0.0524\n",
      "Epoch 16/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 1.0288 - decoder_loss: 0.1133 - pred_loss: 0.9155 - decoder_MAE: 0.1986 - pred_corr: 0.1136 - val_loss: 30.2824 - val_decoder_loss: 0.0817 - val_pred_loss: 30.2007 - val_decoder_MAE: 0.1732 - val_pred_corr: 0.0543\n",
      "Epoch 17/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 1.0104 - decoder_loss: 0.1123 - pred_loss: 0.8982 - decoder_MAE: 0.1969 - pred_corr: 0.1158 - val_loss: 26.8980 - val_decoder_loss: 0.0800 - val_pred_loss: 26.8180 - val_decoder_MAE: 0.1711 - val_pred_corr: 0.0569\n",
      "Epoch 18/100\n",
      "280/280 [==============================] - 7s 23ms/step - loss: 0.9780 - decoder_loss: 0.1103 - pred_loss: 0.8677 - decoder_MAE: 0.1948 - pred_corr: 0.1193 - val_loss: 25.1265 - val_decoder_loss: 0.0789 - val_pred_loss: 25.0476 - val_decoder_MAE: 0.1695 - val_pred_corr: 0.0585\n",
      "Epoch 19/100\n",
      "280/280 [==============================] - 7s 23ms/step - loss: 0.9423 - decoder_loss: 0.1075 - pred_loss: 0.8348 - decoder_MAE: 0.1924 - pred_corr: 0.1240 - val_loss: 28.5461 - val_decoder_loss: 0.0779 - val_pred_loss: 28.4682 - val_decoder_MAE: 0.1667 - val_pred_corr: 0.0598\n",
      "Epoch 20/100\n",
      "280/280 [==============================] - 7s 23ms/step - loss: 0.9173 - decoder_loss: 0.1053 - pred_loss: 0.8120 - decoder_MAE: 0.1900 - pred_corr: 0.1272 - val_loss: 27.1834 - val_decoder_loss: 0.0742 - val_pred_loss: 27.1092 - val_decoder_MAE: 0.1639 - val_pred_corr: 0.0611\n",
      "Epoch 21/100\n",
      "280/280 [==============================] - 7s 23ms/step - loss: 0.8990 - decoder_loss: 0.1042 - pred_loss: 0.7948 - decoder_MAE: 0.1880 - pred_corr: 0.1298 - val_loss: 28.5047 - val_decoder_loss: 0.0720 - val_pred_loss: 28.4327 - val_decoder_MAE: 0.1613 - val_pred_corr: 0.0614\n",
      "Epoch 22/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.8742 - decoder_loss: 0.1016 - pred_loss: 0.7726 - decoder_MAE: 0.1856 - pred_corr: 0.1331 - val_loss: 27.3396 - val_decoder_loss: 0.0703 - val_pred_loss: 27.2693 - val_decoder_MAE: 0.1588 - val_pred_corr: 0.0632\n",
      "Epoch 23/100\n",
      "280/280 [==============================] - 7s 23ms/step - loss: 0.8568 - decoder_loss: 0.0998 - pred_loss: 0.7570 - decoder_MAE: 0.1833 - pred_corr: 0.1361 - val_loss: 26.8712 - val_decoder_loss: 0.0685 - val_pred_loss: 26.8026 - val_decoder_MAE: 0.1569 - val_pred_corr: 0.0631\n",
      "Epoch 24/100\n",
      "280/280 [==============================] - 7s 23ms/step - loss: 0.8370 - decoder_loss: 0.0975 - pred_loss: 0.7395 - decoder_MAE: 0.1812 - pred_corr: 0.1394 - val_loss: 26.6639 - val_decoder_loss: 0.0663 - val_pred_loss: 26.5976 - val_decoder_MAE: 0.1541 - val_pred_corr: 0.0649\n",
      "Epoch 25/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.8223 - decoder_loss: 0.0964 - pred_loss: 0.7259 - decoder_MAE: 0.1794 - pred_corr: 0.1417 - val_loss: 25.5139 - val_decoder_loss: 0.0645 - val_pred_loss: 25.4494 - val_decoder_MAE: 0.1511 - val_pred_corr: 0.0630\n",
      "Epoch 26/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.8030 - decoder_loss: 0.0941 - pred_loss: 0.7089 - decoder_MAE: 0.1771 - pred_corr: 0.1450 - val_loss: 26.7823 - val_decoder_loss: 0.0625 - val_pred_loss: 26.7199 - val_decoder_MAE: 0.1481 - val_pred_corr: 0.0624\n",
      "Epoch 27/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.7925 - decoder_loss: 0.0923 - pred_loss: 0.7001 - decoder_MAE: 0.1748 - pred_corr: 0.1466 - val_loss: 23.2359 - val_decoder_loss: 0.0607 - val_pred_loss: 23.1752 - val_decoder_MAE: 0.1463 - val_pred_corr: 0.0646\n",
      "Epoch 28/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.7766 - decoder_loss: 0.0903 - pred_loss: 0.6863 - decoder_MAE: 0.1727 - pred_corr: 0.1490 - val_loss: 28.4257 - val_decoder_loss: 0.0585 - val_pred_loss: 28.3673 - val_decoder_MAE: 0.1432 - val_pred_corr: 0.0611\n",
      "Epoch 29/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.7634 - decoder_loss: 0.0885 - pred_loss: 0.6750 - decoder_MAE: 0.1707 - pred_corr: 0.1514 - val_loss: 23.6407 - val_decoder_loss: 0.0572 - val_pred_loss: 23.5835 - val_decoder_MAE: 0.1415 - val_pred_corr: 0.0609\n",
      "Epoch 30/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.7466 - decoder_loss: 0.0868 - pred_loss: 0.6598 - decoder_MAE: 0.1686 - pred_corr: 0.1548 - val_loss: 29.7387 - val_decoder_loss: 0.0560 - val_pred_loss: 29.6827 - val_decoder_MAE: 0.1384 - val_pred_corr: 0.0641\n",
      "Epoch 31/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.7395 - decoder_loss: 0.0854 - pred_loss: 0.6541 - decoder_MAE: 0.1667 - pred_corr: 0.1564 - val_loss: 26.0213 - val_decoder_loss: 0.0544 - val_pred_loss: 25.9669 - val_decoder_MAE: 0.1369 - val_pred_corr: 0.0615\n",
      "Epoch 32/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.7272 - decoder_loss: 0.0833 - pred_loss: 0.6439 - decoder_MAE: 0.1647 - pred_corr: 0.1585 - val_loss: 26.0074 - val_decoder_loss: 0.0529 - val_pred_loss: 25.9545 - val_decoder_MAE: 0.1346 - val_pred_corr: 0.0640\n",
      "Epoch 33/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.7166 - decoder_loss: 0.0819 - pred_loss: 0.6348 - decoder_MAE: 0.1630 - pred_corr: 0.1609 - val_loss: 22.0987 - val_decoder_loss: 0.0515 - val_pred_loss: 22.0472 - val_decoder_MAE: 0.1333 - val_pred_corr: 0.0677\n",
      "Epoch 34/100\n",
      "280/280 [==============================] - 7s 23ms/step - loss: 0.6965 - decoder_loss: 0.0807 - pred_loss: 0.6157 - decoder_MAE: 0.1615 - pred_corr: 0.1656 - val_loss: 26.7629 - val_decoder_loss: 0.0499 - val_pred_loss: 26.7130 - val_decoder_MAE: 0.1302 - val_pred_corr: 0.0666\n",
      "Epoch 35/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.6907 - decoder_loss: 0.0796 - pred_loss: 0.6111 - decoder_MAE: 0.1598 - pred_corr: 0.1670 - val_loss: 26.5583 - val_decoder_loss: 0.0493 - val_pred_loss: 26.5090 - val_decoder_MAE: 0.1300 - val_pred_corr: 0.0642\n",
      "Epoch 36/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.6842 - decoder_loss: 0.0784 - pred_loss: 0.6058 - decoder_MAE: 0.1585 - pred_corr: 0.1684 - val_loss: 25.0983 - val_decoder_loss: 0.0479 - val_pred_loss: 25.0505 - val_decoder_MAE: 0.1270 - val_pred_corr: 0.0687\n",
      "Epoch 37/100\n",
      "280/280 [==============================] - 7s 23ms/step - loss: 0.6699 - decoder_loss: 0.0770 - pred_loss: 0.5928 - decoder_MAE: 0.1571 - pred_corr: 0.1719 - val_loss: 26.1332 - val_decoder_loss: 0.0475 - val_pred_loss: 26.0856 - val_decoder_MAE: 0.1251 - val_pred_corr: 0.0679\n",
      "Epoch 38/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.6554 - decoder_loss: 0.0766 - pred_loss: 0.5789 - decoder_MAE: 0.1557 - pred_corr: 0.1755 - val_loss: 23.9722 - val_decoder_loss: 0.0464 - val_pred_loss: 23.9258 - val_decoder_MAE: 0.1241 - val_pred_corr: 0.0694\n",
      "Epoch 39/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.6445 - decoder_loss: 0.0751 - pred_loss: 0.5694 - decoder_MAE: 0.1544 - pred_corr: 0.1783 - val_loss: 24.2652 - val_decoder_loss: 0.0446 - val_pred_loss: 24.2206 - val_decoder_MAE: 0.1218 - val_pred_corr: 0.0675\n",
      "Epoch 40/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.6334 - decoder_loss: 0.0744 - pred_loss: 0.5591 - decoder_MAE: 0.1532 - pred_corr: 0.1814 - val_loss: 27.1678 - val_decoder_loss: 0.0435 - val_pred_loss: 27.1244 - val_decoder_MAE: 0.1197 - val_pred_corr: 0.0730\n",
      "Epoch 41/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.6233 - decoder_loss: 0.0732 - pred_loss: 0.5500 - decoder_MAE: 0.1519 - pred_corr: 0.1848 - val_loss: 23.3626 - val_decoder_loss: 0.0431 - val_pred_loss: 23.3195 - val_decoder_MAE: 0.1189 - val_pred_corr: 0.0721\n",
      "Epoch 42/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.6216 - decoder_loss: 0.0722 - pred_loss: 0.5494 - decoder_MAE: 0.1507 - pred_corr: 0.1852 - val_loss: 25.6235 - val_decoder_loss: 0.0435 - val_pred_loss: 25.5801 - val_decoder_MAE: 0.1190 - val_pred_corr: 0.0700\n",
      "Epoch 43/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.6134 - decoder_loss: 0.0720 - pred_loss: 0.5414 - decoder_MAE: 0.1498 - pred_corr: 0.1878 - val_loss: 24.0146 - val_decoder_loss: 0.0425 - val_pred_loss: 23.9721 - val_decoder_MAE: 0.1170 - val_pred_corr: 0.0739\n",
      "Epoch 44/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.5971 - decoder_loss: 0.0709 - pred_loss: 0.5262 - decoder_MAE: 0.1486 - pred_corr: 0.1929 - val_loss: 25.0974 - val_decoder_loss: 0.0404 - val_pred_loss: 25.0570 - val_decoder_MAE: 0.1152 - val_pred_corr: 0.0718\n",
      "Epoch 45/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.5850 - decoder_loss: 0.0696 - pred_loss: 0.5154 - decoder_MAE: 0.1472 - pred_corr: 0.1968 - val_loss: 21.8455 - val_decoder_loss: 0.0401 - val_pred_loss: 21.8054 - val_decoder_MAE: 0.1144 - val_pred_corr: 0.0719\n",
      "Epoch 46/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.5808 - decoder_loss: 0.0686 - pred_loss: 0.5122 - decoder_MAE: 0.1459 - pred_corr: 0.1980 - val_loss: 26.9263 - val_decoder_loss: 0.0393 - val_pred_loss: 26.8870 - val_decoder_MAE: 0.1128 - val_pred_corr: 0.0683\n",
      "Epoch 47/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.5705 - decoder_loss: 0.0677 - pred_loss: 0.5028 - decoder_MAE: 0.1451 - pred_corr: 0.2016 - val_loss: 28.8685 - val_decoder_loss: 0.0383 - val_pred_loss: 28.8302 - val_decoder_MAE: 0.1114 - val_pred_corr: 0.0739\n",
      "Epoch 48/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.5618 - decoder_loss: 0.0668 - pred_loss: 0.4950 - decoder_MAE: 0.1440 - pred_corr: 0.2048 - val_loss: 25.4232 - val_decoder_loss: 0.0373 - val_pred_loss: 25.3858 - val_decoder_MAE: 0.1098 - val_pred_corr: 0.0679\n",
      "Epoch 49/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.5562 - decoder_loss: 0.0660 - pred_loss: 0.4903 - decoder_MAE: 0.1431 - pred_corr: 0.2065 - val_loss: 30.2015 - val_decoder_loss: 0.0379 - val_pred_loss: 30.1636 - val_decoder_MAE: 0.1093 - val_pred_corr: 0.0671\n",
      "Epoch 50/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.5500 - decoder_loss: 0.0653 - pred_loss: 0.4847 - decoder_MAE: 0.1420 - pred_corr: 0.2090 - val_loss: 23.4551 - val_decoder_loss: 0.0364 - val_pred_loss: 23.4187 - val_decoder_MAE: 0.1075 - val_pred_corr: 0.0745\n",
      "Epoch 51/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.5410 - decoder_loss: 0.0649 - pred_loss: 0.4761 - decoder_MAE: 0.1414 - pred_corr: 0.2128 - val_loss: 29.9619 - val_decoder_loss: 0.0357 - val_pred_loss: 29.9262 - val_decoder_MAE: 0.1061 - val_pred_corr: 0.0686\n",
      "Epoch 52/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.5327 - decoder_loss: 0.0642 - pred_loss: 0.4686 - decoder_MAE: 0.1404 - pred_corr: 0.2159 - val_loss: 27.5084 - val_decoder_loss: 0.0355 - val_pred_loss: 27.4728 - val_decoder_MAE: 0.1050 - val_pred_corr: 0.0673\n",
      "Epoch 53/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.5299 - decoder_loss: 0.0639 - pred_loss: 0.4659 - decoder_MAE: 0.1398 - pred_corr: 0.2171 - val_loss: 24.8550 - val_decoder_loss: 0.0352 - val_pred_loss: 24.8197 - val_decoder_MAE: 0.1060 - val_pred_corr: 0.0708\n",
      "Epoch 54/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.5204 - decoder_loss: 0.0629 - pred_loss: 0.4575 - decoder_MAE: 0.1389 - pred_corr: 0.2210 - val_loss: 26.5965 - val_decoder_loss: 0.0336 - val_pred_loss: 26.5629 - val_decoder_MAE: 0.1023 - val_pred_corr: 0.0650\n",
      "Epoch 55/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.5157 - decoder_loss: 0.0623 - pred_loss: 0.4534 - decoder_MAE: 0.1380 - pred_corr: 0.2231 - val_loss: 27.5226 - val_decoder_loss: 0.0336 - val_pred_loss: 27.4890 - val_decoder_MAE: 0.1022 - val_pred_corr: 0.0643\n",
      "Epoch 56/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.5084 - decoder_loss: 0.0621 - pred_loss: 0.4463 - decoder_MAE: 0.1374 - pred_corr: 0.2265 - val_loss: 24.7330 - val_decoder_loss: 0.0336 - val_pred_loss: 24.6995 - val_decoder_MAE: 0.1014 - val_pred_corr: 0.0680\n",
      "Epoch 57/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.5032 - decoder_loss: 0.0621 - pred_loss: 0.4411 - decoder_MAE: 0.1368 - pred_corr: 0.2292 - val_loss: 21.8307 - val_decoder_loss: 0.0325 - val_pred_loss: 21.7981 - val_decoder_MAE: 0.0996 - val_pred_corr: 0.0702\n",
      "Epoch 58/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.4955 - decoder_loss: 0.0610 - pred_loss: 0.4346 - decoder_MAE: 0.1359 - pred_corr: 0.2325 - val_loss: 27.1175 - val_decoder_loss: 0.0326 - val_pred_loss: 27.0849 - val_decoder_MAE: 0.0999 - val_pred_corr: 0.0648\n",
      "Epoch 59/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.4901 - decoder_loss: 0.0604 - pred_loss: 0.4296 - decoder_MAE: 0.1352 - pred_corr: 0.2354 - val_loss: 30.2871 - val_decoder_loss: 0.0322 - val_pred_loss: 30.2549 - val_decoder_MAE: 0.0997 - val_pred_corr: 0.0681\n",
      "Epoch 60/100\n",
      "280/280 [==============================] - 6s 23ms/step - loss: 0.4841 - decoder_loss: 0.0605 - pred_loss: 0.4236 - decoder_MAE: 0.1349 - pred_corr: 0.2386 - val_loss: 25.3006 - val_decoder_loss: 0.0323 - val_pred_loss: 25.2682 - val_decoder_MAE: 0.0983 - val_pred_corr: 0.0643\n",
      "Fold 3 val_correlation:\t 0.07448587566614151\n",
      "Epoch 1/100\n",
      "356/356 [==============================] - 13s 24ms/step - loss: 6.3012 - decoder_loss: 0.3257 - pred_loss: 5.9755 - decoder_MAE: 0.3293 - pred_corr: 0.0366 - val_loss: 20.5441 - val_decoder_loss: 0.1605 - val_pred_loss: 20.3836 - val_decoder_MAE: 0.2498 - val_pred_corr: 0.0565\n",
      "Epoch 2/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 4.1679 - decoder_loss: 0.1784 - pred_loss: 3.9894 - decoder_MAE: 0.2559 - pred_corr: 0.0439 - val_loss: 16.5552 - val_decoder_loss: 0.1305 - val_pred_loss: 16.4247 - val_decoder_MAE: 0.2247 - val_pred_corr: 0.0575\n",
      "Epoch 3/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 4.6776 - decoder_loss: 0.1601 - pred_loss: 4.5176 - decoder_MAE: 0.2419 - pred_corr: 0.0428 - val_loss: 19.6624 - val_decoder_loss: 0.1180 - val_pred_loss: 19.5443 - val_decoder_MAE: 0.2149 - val_pred_corr: 0.0518\n",
      "Epoch 4/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 2.6344 - decoder_loss: 0.1485 - pred_loss: 2.4860 - decoder_MAE: 0.2333 - pred_corr: 0.0524 - val_loss: 13.3651 - val_decoder_loss: 0.1099 - val_pred_loss: 13.2552 - val_decoder_MAE: 0.2067 - val_pred_corr: 0.0590\n",
      "Epoch 5/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 2.2234 - decoder_loss: 0.1419 - pred_loss: 2.0814 - decoder_MAE: 0.2277 - pred_corr: 0.0583 - val_loss: 13.9073 - val_decoder_loss: 0.1045 - val_pred_loss: 13.8029 - val_decoder_MAE: 0.2019 - val_pred_corr: 0.0609\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356/356 [==============================] - 8s 22ms/step - loss: 2.0607 - decoder_loss: 0.1372 - pred_loss: 1.9236 - decoder_MAE: 0.2237 - pred_corr: 0.0614 - val_loss: 16.3285 - val_decoder_loss: 0.1009 - val_pred_loss: 16.2276 - val_decoder_MAE: 0.1983 - val_pred_corr: 0.0594\n",
      "Epoch 7/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 1.8588 - decoder_loss: 0.1336 - pred_loss: 1.7253 - decoder_MAE: 0.2206 - pred_corr: 0.0658 - val_loss: 15.0404 - val_decoder_loss: 0.0975 - val_pred_loss: 14.9430 - val_decoder_MAE: 0.1948 - val_pred_corr: 0.0612\n",
      "Epoch 8/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 1.8110 - decoder_loss: 0.1310 - pred_loss: 1.6800 - decoder_MAE: 0.2178 - pred_corr: 0.0687 - val_loss: 17.5419 - val_decoder_loss: 0.0943 - val_pred_loss: 17.4476 - val_decoder_MAE: 0.1917 - val_pred_corr: 0.0564\n",
      "Epoch 9/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 1.6398 - decoder_loss: 0.1283 - pred_loss: 1.5115 - decoder_MAE: 0.2149 - pred_corr: 0.0740 - val_loss: 15.0472 - val_decoder_loss: 0.0921 - val_pred_loss: 14.9552 - val_decoder_MAE: 0.1886 - val_pred_corr: 0.0597\n",
      "Epoch 10/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 1.5299 - decoder_loss: 0.1252 - pred_loss: 1.4047 - decoder_MAE: 0.2121 - pred_corr: 0.0781 - val_loss: 15.2051 - val_decoder_loss: 0.0898 - val_pred_loss: 15.1152 - val_decoder_MAE: 0.1863 - val_pred_corr: 0.0571\n",
      "Epoch 11/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 1.4391 - decoder_loss: 0.1230 - pred_loss: 1.3161 - decoder_MAE: 0.2097 - pred_corr: 0.0824 - val_loss: 15.0220 - val_decoder_loss: 0.0879 - val_pred_loss: 14.9341 - val_decoder_MAE: 0.1832 - val_pred_corr: 0.0591\n",
      "Epoch 12/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 1.3821 - decoder_loss: 0.1216 - pred_loss: 1.2606 - decoder_MAE: 0.2076 - pred_corr: 0.0856 - val_loss: 17.4105 - val_decoder_loss: 0.0859 - val_pred_loss: 17.3246 - val_decoder_MAE: 0.1811 - val_pred_corr: 0.0587\n",
      "Epoch 13/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 1.3256 - decoder_loss: 0.1197 - pred_loss: 1.2059 - decoder_MAE: 0.2055 - pred_corr: 0.0888 - val_loss: 15.9889 - val_decoder_loss: 0.0839 - val_pred_loss: 15.9050 - val_decoder_MAE: 0.1785 - val_pred_corr: 0.0608\n",
      "Epoch 14/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 1.2669 - decoder_loss: 0.1169 - pred_loss: 1.1501 - decoder_MAE: 0.2029 - pred_corr: 0.0924 - val_loss: 15.4294 - val_decoder_loss: 0.0820 - val_pred_loss: 15.3474 - val_decoder_MAE: 0.1758 - val_pred_corr: 0.0614\n",
      "Epoch 15/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 1.2106 - decoder_loss: 0.1161 - pred_loss: 1.0945 - decoder_MAE: 0.2010 - pred_corr: 0.0963 - val_loss: 14.0666 - val_decoder_loss: 0.0818 - val_pred_loss: 13.9848 - val_decoder_MAE: 0.1747 - val_pred_corr: 0.0629\n",
      "Epoch 16/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 1.1765 - decoder_loss: 0.1141 - pred_loss: 1.0624 - decoder_MAE: 0.1986 - pred_corr: 0.0992 - val_loss: 15.9860 - val_decoder_loss: 0.0796 - val_pred_loss: 15.9064 - val_decoder_MAE: 0.1722 - val_pred_corr: 0.0608\n",
      "Epoch 17/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 1.1440 - decoder_loss: 0.1112 - pred_loss: 1.0328 - decoder_MAE: 0.1959 - pred_corr: 0.1017 - val_loss: 16.4513 - val_decoder_loss: 0.0762 - val_pred_loss: 16.3751 - val_decoder_MAE: 0.1684 - val_pred_corr: 0.0605\n",
      "Epoch 18/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 1.1276 - decoder_loss: 0.1088 - pred_loss: 1.0187 - decoder_MAE: 0.1935 - pred_corr: 0.1038 - val_loss: 17.7858 - val_decoder_loss: 0.0752 - val_pred_loss: 17.7106 - val_decoder_MAE: 0.1658 - val_pred_corr: 0.0607\n",
      "Epoch 19/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 1.1013 - decoder_loss: 0.1069 - pred_loss: 0.9944 - decoder_MAE: 0.1914 - pred_corr: 0.1081 - val_loss: 16.7625 - val_decoder_loss: 0.0728 - val_pred_loss: 16.6897 - val_decoder_MAE: 0.1632 - val_pred_corr: 0.0600\n",
      "Epoch 20/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 1.0473 - decoder_loss: 0.1045 - pred_loss: 0.9428 - decoder_MAE: 0.1886 - pred_corr: 0.1106 - val_loss: 15.8867 - val_decoder_loss: 0.0715 - val_pred_loss: 15.8152 - val_decoder_MAE: 0.1613 - val_pred_corr: 0.0618\n",
      "Epoch 21/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 1.0153 - decoder_loss: 0.1024 - pred_loss: 0.9130 - decoder_MAE: 0.1861 - pred_corr: 0.1141 - val_loss: 13.7364 - val_decoder_loss: 0.0708 - val_pred_loss: 13.6657 - val_decoder_MAE: 0.1587 - val_pred_corr: 0.0636\n",
      "Epoch 22/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 0.9882 - decoder_loss: 0.1004 - pred_loss: 0.8878 - decoder_MAE: 0.1835 - pred_corr: 0.1176 - val_loss: 15.9990 - val_decoder_loss: 0.0669 - val_pred_loss: 15.9321 - val_decoder_MAE: 0.1550 - val_pred_corr: 0.0574\n",
      "Epoch 23/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 0.9461 - decoder_loss: 0.0980 - pred_loss: 0.8481 - decoder_MAE: 0.1808 - pred_corr: 0.1219 - val_loss: 17.5159 - val_decoder_loss: 0.0645 - val_pred_loss: 17.4514 - val_decoder_MAE: 0.1515 - val_pred_corr: 0.0622\n",
      "Epoch 24/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 0.9170 - decoder_loss: 0.0960 - pred_loss: 0.8209 - decoder_MAE: 0.1784 - pred_corr: 0.1256 - val_loss: 13.3799 - val_decoder_loss: 0.0623 - val_pred_loss: 13.3176 - val_decoder_MAE: 0.1486 - val_pred_corr: 0.0632\n",
      "Epoch 25/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 0.8929 - decoder_loss: 0.0939 - pred_loss: 0.7990 - decoder_MAE: 0.1758 - pred_corr: 0.1292 - val_loss: 20.7996 - val_decoder_loss: 0.0606 - val_pred_loss: 20.7391 - val_decoder_MAE: 0.1459 - val_pred_corr: 0.0575\n",
      "Epoch 26/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 0.9600 - decoder_loss: 0.0946 - pred_loss: 0.8654 - decoder_MAE: 0.1759 - pred_corr: 0.1214 - val_loss: 16.7586 - val_decoder_loss: 0.0601 - val_pred_loss: 16.6985 - val_decoder_MAE: 0.1457 - val_pred_corr: 0.0643\n",
      "Epoch 27/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 0.8620 - decoder_loss: 0.0910 - pred_loss: 0.7710 - decoder_MAE: 0.1723 - pred_corr: 0.1339 - val_loss: 18.6861 - val_decoder_loss: 0.0574 - val_pred_loss: 18.6288 - val_decoder_MAE: 0.1414 - val_pred_corr: 0.0623\n",
      "Epoch 28/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 0.8309 - decoder_loss: 0.0883 - pred_loss: 0.7426 - decoder_MAE: 0.1693 - pred_corr: 0.1383 - val_loss: 17.0776 - val_decoder_loss: 0.0553 - val_pred_loss: 17.0223 - val_decoder_MAE: 0.1381 - val_pred_corr: 0.0694\n",
      "Epoch 29/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 0.8062 - decoder_loss: 0.0868 - pred_loss: 0.7195 - decoder_MAE: 0.1671 - pred_corr: 0.1429 - val_loss: 19.0432 - val_decoder_loss: 0.0531 - val_pred_loss: 18.9901 - val_decoder_MAE: 0.1349 - val_pred_corr: 0.0669\n",
      "Epoch 30/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 0.7862 - decoder_loss: 0.0846 - pred_loss: 0.7016 - decoder_MAE: 0.1647 - pred_corr: 0.1462 - val_loss: 19.2630 - val_decoder_loss: 0.0522 - val_pred_loss: 19.2108 - val_decoder_MAE: 0.1332 - val_pred_corr: 0.0673\n",
      "Epoch 31/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 0.7818 - decoder_loss: 0.0832 - pred_loss: 0.6986 - decoder_MAE: 0.1631 - pred_corr: 0.1469 - val_loss: 19.6687 - val_decoder_loss: 0.0503 - val_pred_loss: 19.6184 - val_decoder_MAE: 0.1313 - val_pred_corr: 0.0662\n",
      "Epoch 32/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 0.7562 - decoder_loss: 0.0814 - pred_loss: 0.6748 - decoder_MAE: 0.1611 - pred_corr: 0.1521 - val_loss: 18.8869 - val_decoder_loss: 0.0487 - val_pred_loss: 18.8383 - val_decoder_MAE: 0.1282 - val_pred_corr: 0.0672\n",
      "Epoch 33/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 0.7354 - decoder_loss: 0.0792 - pred_loss: 0.6562 - decoder_MAE: 0.1589 - pred_corr: 0.1555 - val_loss: 21.2400 - val_decoder_loss: 0.0460 - val_pred_loss: 21.1940 - val_decoder_MAE: 0.1246 - val_pred_corr: 0.0653\n",
      "Epoch 34/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 0.7207 - decoder_loss: 0.0773 - pred_loss: 0.6435 - decoder_MAE: 0.1569 - pred_corr: 0.1586 - val_loss: 20.1029 - val_decoder_loss: 0.0445 - val_pred_loss: 20.0584 - val_decoder_MAE: 0.1223 - val_pred_corr: 0.0658\n",
      "Epoch 35/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 0.7131 - decoder_loss: 0.0765 - pred_loss: 0.6366 - decoder_MAE: 0.1557 - pred_corr: 0.1604 - val_loss: 20.6259 - val_decoder_loss: 0.0449 - val_pred_loss: 20.5810 - val_decoder_MAE: 0.1225 - val_pred_corr: 0.0679\n",
      "Epoch 36/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 0.7010 - decoder_loss: 0.0749 - pred_loss: 0.6261 - decoder_MAE: 0.1540 - pred_corr: 0.1625 - val_loss: 19.7991 - val_decoder_loss: 0.0432 - val_pred_loss: 19.7560 - val_decoder_MAE: 0.1198 - val_pred_corr: 0.0716\n",
      "Epoch 37/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 0.6901 - decoder_loss: 0.0735 - pred_loss: 0.6166 - decoder_MAE: 0.1522 - pred_corr: 0.1653 - val_loss: 21.2442 - val_decoder_loss: 0.0421 - val_pred_loss: 21.2021 - val_decoder_MAE: 0.1185 - val_pred_corr: 0.0694\n",
      "Epoch 38/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 0.6823 - decoder_loss: 0.0725 - pred_loss: 0.6098 - decoder_MAE: 0.1510 - pred_corr: 0.1672 - val_loss: 19.7858 - val_decoder_loss: 0.0406 - val_pred_loss: 19.7452 - val_decoder_MAE: 0.1160 - val_pred_corr: 0.0733\n",
      "Epoch 39/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 0.6689 - decoder_loss: 0.0717 - pred_loss: 0.5972 - decoder_MAE: 0.1499 - pred_corr: 0.1705 - val_loss: 19.5513 - val_decoder_loss: 0.0397 - val_pred_loss: 19.5116 - val_decoder_MAE: 0.1150 - val_pred_corr: 0.0698\n",
      "Epoch 40/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 0.6586 - decoder_loss: 0.0708 - pred_loss: 0.5878 - decoder_MAE: 0.1484 - pred_corr: 0.1733 - val_loss: 19.2970 - val_decoder_loss: 0.0395 - val_pred_loss: 19.2575 - val_decoder_MAE: 0.1139 - val_pred_corr: 0.0695\n",
      "Epoch 41/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 0.6466 - decoder_loss: 0.0698 - pred_loss: 0.5768 - decoder_MAE: 0.1472 - pred_corr: 0.1765 - val_loss: 21.3734 - val_decoder_loss: 0.0383 - val_pred_loss: 21.3351 - val_decoder_MAE: 0.1119 - val_pred_corr: 0.0606\n",
      "Epoch 42/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 0.6343 - decoder_loss: 0.0687 - pred_loss: 0.5657 - decoder_MAE: 0.1460 - pred_corr: 0.1799 - val_loss: 19.5741 - val_decoder_loss: 0.0388 - val_pred_loss: 19.5353 - val_decoder_MAE: 0.1126 - val_pred_corr: 0.0716\n",
      "Epoch 43/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 0.6296 - decoder_loss: 0.0683 - pred_loss: 0.5613 - decoder_MAE: 0.1449 - pred_corr: 0.1810 - val_loss: 24.0354 - val_decoder_loss: 0.0378 - val_pred_loss: 23.9976 - val_decoder_MAE: 0.1097 - val_pred_corr: 0.0542\n",
      "Epoch 44/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 0.6508 - decoder_loss: 0.0696 - pred_loss: 0.5812 - decoder_MAE: 0.1463 - pred_corr: 0.1755 - val_loss: 24.0881 - val_decoder_loss: 0.0375 - val_pred_loss: 24.0506 - val_decoder_MAE: 0.1111 - val_pred_corr: 0.0677\n",
      "Epoch 45/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 0.6274 - decoder_loss: 0.0696 - pred_loss: 0.5578 - decoder_MAE: 0.1454 - pred_corr: 0.1823 - val_loss: 18.8294 - val_decoder_loss: 0.0366 - val_pred_loss: 18.7928 - val_decoder_MAE: 0.1071 - val_pred_corr: 0.0709\n",
      "Epoch 46/100\n",
      "356/356 [==============================] - 8s 23ms/step - loss: 0.6040 - decoder_loss: 0.0665 - pred_loss: 0.5376 - decoder_MAE: 0.1422 - pred_corr: 0.1888 - val_loss: 18.9959 - val_decoder_loss: 0.0352 - val_pred_loss: 18.9607 - val_decoder_MAE: 0.1056 - val_pred_corr: 0.0701\n",
      "Epoch 47/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 0.5939 - decoder_loss: 0.0650 - pred_loss: 0.5289 - decoder_MAE: 0.1407 - pred_corr: 0.1920 - val_loss: 19.0194 - val_decoder_loss: 0.0343 - val_pred_loss: 18.9851 - val_decoder_MAE: 0.1035 - val_pred_corr: 0.0694\n",
      "Epoch 48/100\n",
      "356/356 [==============================] - 8s 22ms/step - loss: 0.5822 - decoder_loss: 0.0637 - pred_loss: 0.5185 - decoder_MAE: 0.1393 - pred_corr: 0.1952 - val_loss: 23.8315 - val_decoder_loss: 0.0336 - val_pred_loss: 23.7979 - val_decoder_MAE: 0.1022 - val_pred_corr: 0.0628\n",
      "Fold 4 val_correlation:\t 0.07333409041166306\n",
      "Epoch 1/100\n",
      "436/436 [==============================] - 14s 23ms/step - loss: 5.1797 - decoder_loss: 0.2965 - pred_loss: 4.8833 - decoder_MAE: 0.3161 - pred_corr: 0.0427 - val_loss: 32.1077 - val_decoder_loss: 0.1532 - val_pred_loss: 31.9545 - val_decoder_MAE: 0.2468 - val_pred_corr: 0.0293\n",
      "Epoch 2/100\n",
      "436/436 [==============================] - 10s 22ms/step - loss: 2.9134 - decoder_loss: 0.1671 - pred_loss: 2.7463 - decoder_MAE: 0.2481 - pred_corr: 0.0547 - val_loss: 22.7970 - val_decoder_loss: 0.1252 - val_pred_loss: 22.6719 - val_decoder_MAE: 0.2231 - val_pred_corr: 0.0406\n",
      "Epoch 3/100\n",
      "436/436 [==============================] - 10s 22ms/step - loss: 2.0219 - decoder_loss: 0.1501 - pred_loss: 1.8719 - decoder_MAE: 0.2344 - pred_corr: 0.0635 - val_loss: 25.5918 - val_decoder_loss: 0.1158 - val_pred_loss: 25.4760 - val_decoder_MAE: 0.2132 - val_pred_corr: 0.0418\n",
      "Epoch 4/100\n",
      "436/436 [==============================] - 10s 22ms/step - loss: 1.7832 - decoder_loss: 0.1425 - pred_loss: 1.6406 - decoder_MAE: 0.2273 - pred_corr: 0.0701 - val_loss: 24.2734 - val_decoder_loss: 0.1066 - val_pred_loss: 24.1668 - val_decoder_MAE: 0.2047 - val_pred_corr: 0.0482\n",
      "Epoch 5/100\n",
      "436/436 [==============================] - 9s 22ms/step - loss: 1.5526 - decoder_loss: 0.1359 - pred_loss: 1.4168 - decoder_MAE: 0.2211 - pred_corr: 0.0779 - val_loss: 23.2770 - val_decoder_loss: 0.1013 - val_pred_loss: 23.1757 - val_decoder_MAE: 0.1983 - val_pred_corr: 0.0493\n",
      "Epoch 6/100\n",
      "436/436 [==============================] - 10s 22ms/step - loss: 1.4454 - decoder_loss: 0.1314 - pred_loss: 1.3141 - decoder_MAE: 0.2163 - pred_corr: 0.0828 - val_loss: 26.0739 - val_decoder_loss: 0.0957 - val_pred_loss: 25.9781 - val_decoder_MAE: 0.1925 - val_pred_corr: 0.0493\n",
      "Epoch 7/100\n",
      "436/436 [==============================] - 10s 22ms/step - loss: 1.3424 - decoder_loss: 0.1256 - pred_loss: 1.2167 - decoder_MAE: 0.2113 - pred_corr: 0.0884 - val_loss: 25.4144 - val_decoder_loss: 0.0916 - val_pred_loss: 25.3229 - val_decoder_MAE: 0.1876 - val_pred_corr: 0.0486\n",
      "Epoch 8/100\n",
      "436/436 [==============================] - 10s 22ms/step - loss: 1.3485 - decoder_loss: 0.1218 - pred_loss: 1.2267 - decoder_MAE: 0.2078 - pred_corr: 0.0883 - val_loss: 25.9195 - val_decoder_loss: 0.0885 - val_pred_loss: 25.8310 - val_decoder_MAE: 0.1831 - val_pred_corr: 0.0471\n",
      "Epoch 9/100\n",
      "436/436 [==============================] - 10s 22ms/step - loss: 1.2382 - decoder_loss: 0.1172 - pred_loss: 1.1210 - decoder_MAE: 0.2030 - pred_corr: 0.0953 - val_loss: 26.1371 - val_decoder_loss: 0.0825 - val_pred_loss: 26.0546 - val_decoder_MAE: 0.1775 - val_pred_corr: 0.0491\n",
      "Epoch 10/100\n",
      "436/436 [==============================] - 10s 22ms/step - loss: 1.1806 - decoder_loss: 0.1123 - pred_loss: 1.0682 - decoder_MAE: 0.1981 - pred_corr: 0.0991 - val_loss: 25.9838 - val_decoder_loss: 0.0794 - val_pred_loss: 25.9045 - val_decoder_MAE: 0.1730 - val_pred_corr: 0.0479\n",
      "Epoch 11/100\n",
      "436/436 [==============================] - 10s 22ms/step - loss: 1.2402 - decoder_loss: 0.1107 - pred_loss: 1.1295 - decoder_MAE: 0.1958 - pred_corr: 0.0961 - val_loss: 24.0232 - val_decoder_loss: 0.0766 - val_pred_loss: 23.9465 - val_decoder_MAE: 0.1697 - val_pred_corr: 0.0518\n",
      "Epoch 12/100\n",
      "436/436 [==============================] - 10s 22ms/step - loss: 1.1151 - decoder_loss: 0.1057 - pred_loss: 1.0094 - decoder_MAE: 0.1906 - pred_corr: 0.1047 - val_loss: 23.9689 - val_decoder_loss: 0.0742 - val_pred_loss: 23.8948 - val_decoder_MAE: 0.1654 - val_pred_corr: 0.0518\n",
      "Epoch 13/100\n",
      "436/436 [==============================] - 10s 22ms/step - loss: 1.0745 - decoder_loss: 0.1016 - pred_loss: 0.9730 - decoder_MAE: 0.1864 - pred_corr: 0.1090 - val_loss: 23.1864 - val_decoder_loss: 0.0700 - val_pred_loss: 23.1164 - val_decoder_MAE: 0.1612 - val_pred_corr: 0.0545\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436/436 [==============================] - 10s 22ms/step - loss: 1.0326 - decoder_loss: 0.0990 - pred_loss: 0.9337 - decoder_MAE: 0.1832 - pred_corr: 0.1122 - val_loss: 23.9209 - val_decoder_loss: 0.0665 - val_pred_loss: 23.8544 - val_decoder_MAE: 0.1563 - val_pred_corr: 0.0531\n",
      "Epoch 15/100\n",
      "436/436 [==============================] - 10s 22ms/step - loss: 1.0004 - decoder_loss: 0.0958 - pred_loss: 0.9046 - decoder_MAE: 0.1793 - pred_corr: 0.1159 - val_loss: 24.5115 - val_decoder_loss: 0.0645 - val_pred_loss: 24.4469 - val_decoder_MAE: 0.1518 - val_pred_corr: 0.0544\n",
      "Epoch 16/100\n",
      "436/436 [==============================] - 10s 22ms/step - loss: 0.9717 - decoder_loss: 0.0925 - pred_loss: 0.8791 - decoder_MAE: 0.1758 - pred_corr: 0.1191 - val_loss: 21.1456 - val_decoder_loss: 0.0611 - val_pred_loss: 21.0845 - val_decoder_MAE: 0.1484 - val_pred_corr: 0.0543\n",
      "Epoch 17/100\n",
      "436/436 [==============================] - 9s 22ms/step - loss: 0.9272 - decoder_loss: 0.0899 - pred_loss: 0.8373 - decoder_MAE: 0.1725 - pred_corr: 0.1239 - val_loss: 23.5596 - val_decoder_loss: 0.0583 - val_pred_loss: 23.5013 - val_decoder_MAE: 0.1434 - val_pred_corr: 0.0546\n",
      "Epoch 18/100\n",
      "436/436 [==============================] - 10s 22ms/step - loss: 0.8963 - decoder_loss: 0.0871 - pred_loss: 0.8092 - decoder_MAE: 0.1694 - pred_corr: 0.1288 - val_loss: 24.3592 - val_decoder_loss: 0.0572 - val_pred_loss: 24.3020 - val_decoder_MAE: 0.1427 - val_pred_corr: 0.0532\n",
      "Epoch 19/100\n",
      "436/436 [==============================] - 10s 22ms/step - loss: 0.8741 - decoder_loss: 0.0850 - pred_loss: 0.7891 - decoder_MAE: 0.1669 - pred_corr: 0.1312 - val_loss: 24.3971 - val_decoder_loss: 0.0545 - val_pred_loss: 24.3426 - val_decoder_MAE: 0.1386 - val_pred_corr: 0.0531\n",
      "Epoch 20/100\n",
      "436/436 [==============================] - 10s 22ms/step - loss: 0.8354 - decoder_loss: 0.0825 - pred_loss: 0.7529 - decoder_MAE: 0.1640 - pred_corr: 0.1365 - val_loss: 25.9469 - val_decoder_loss: 0.0512 - val_pred_loss: 25.8957 - val_decoder_MAE: 0.1336 - val_pred_corr: 0.0519\n",
      "Epoch 21/100\n",
      "436/436 [==============================] - 9s 22ms/step - loss: 0.8260 - decoder_loss: 0.0804 - pred_loss: 0.7456 - decoder_MAE: 0.1616 - pred_corr: 0.1382 - val_loss: 23.3061 - val_decoder_loss: 0.0494 - val_pred_loss: 23.2567 - val_decoder_MAE: 0.1311 - val_pred_corr: 0.0509\n",
      "Epoch 22/100\n",
      "436/436 [==============================] - 10s 22ms/step - loss: 0.8000 - decoder_loss: 0.0787 - pred_loss: 0.7214 - decoder_MAE: 0.1596 - pred_corr: 0.1421 - val_loss: 21.8446 - val_decoder_loss: 0.0490 - val_pred_loss: 21.7956 - val_decoder_MAE: 0.1291 - val_pred_corr: 0.0569\n",
      "Epoch 23/100\n",
      "436/436 [==============================] - 10s 22ms/step - loss: 0.8653 - decoder_loss: 0.0810 - pred_loss: 0.7844 - decoder_MAE: 0.1616 - pred_corr: 0.1318 - val_loss: 23.9171 - val_decoder_loss: 0.0487 - val_pred_loss: 23.8684 - val_decoder_MAE: 0.1294 - val_pred_corr: 0.0494\n",
      "Epoch 24/100\n",
      "436/436 [==============================] - 9s 22ms/step - loss: 0.8182 - decoder_loss: 0.0782 - pred_loss: 0.7399 - decoder_MAE: 0.1589 - pred_corr: 0.1398 - val_loss: 22.9220 - val_decoder_loss: 0.0466 - val_pred_loss: 22.8754 - val_decoder_MAE: 0.1266 - val_pred_corr: 0.0503\n",
      "Epoch 25/100\n",
      "436/436 [==============================] - 10s 22ms/step - loss: 0.7684 - decoder_loss: 0.0754 - pred_loss: 0.6930 - decoder_MAE: 0.1553 - pred_corr: 0.1476 - val_loss: 23.0878 - val_decoder_loss: 0.0462 - val_pred_loss: 23.0415 - val_decoder_MAE: 0.1249 - val_pred_corr: 0.0511\n",
      "Epoch 26/100\n",
      "436/436 [==============================] - 10s 22ms/step - loss: 0.7763 - decoder_loss: 0.0752 - pred_loss: 0.7012 - decoder_MAE: 0.1547 - pred_corr: 0.1466 - val_loss: 26.8605 - val_decoder_loss: 0.0442 - val_pred_loss: 26.8163 - val_decoder_MAE: 0.1213 - val_pred_corr: 0.0494\n",
      "Epoch 27/100\n",
      "436/436 [==============================] - 10s 22ms/step - loss: 0.7401 - decoder_loss: 0.0725 - pred_loss: 0.6677 - decoder_MAE: 0.1516 - pred_corr: 0.1532 - val_loss: 26.5286 - val_decoder_loss: 0.0421 - val_pred_loss: 26.4865 - val_decoder_MAE: 0.1182 - val_pred_corr: 0.0520\n",
      "Epoch 28/100\n",
      "436/436 [==============================] - 10s 22ms/step - loss: 0.7300 - decoder_loss: 0.0714 - pred_loss: 0.6586 - decoder_MAE: 0.1501 - pred_corr: 0.1557 - val_loss: 24.1573 - val_decoder_loss: 0.0417 - val_pred_loss: 24.1157 - val_decoder_MAE: 0.1183 - val_pred_corr: 0.0504\n",
      "Epoch 29/100\n",
      "436/436 [==============================] - 9s 22ms/step - loss: 0.7115 - decoder_loss: 0.0697 - pred_loss: 0.6418 - decoder_MAE: 0.1481 - pred_corr: 0.1592 - val_loss: 25.5459 - val_decoder_loss: 0.0408 - val_pred_loss: 25.5051 - val_decoder_MAE: 0.1155 - val_pred_corr: 0.0504\n",
      "Epoch 30/100\n",
      "436/436 [==============================] - 10s 22ms/step - loss: 0.6955 - decoder_loss: 0.0683 - pred_loss: 0.6272 - decoder_MAE: 0.1466 - pred_corr: 0.1621 - val_loss: 24.6419 - val_decoder_loss: 0.0403 - val_pred_loss: 24.6017 - val_decoder_MAE: 0.1152 - val_pred_corr: 0.0488\n",
      "Epoch 31/100\n",
      "436/436 [==============================] - 10s 22ms/step - loss: 0.7153 - decoder_loss: 0.0697 - pred_loss: 0.6456 - decoder_MAE: 0.1472 - pred_corr: 0.1588 - val_loss: 27.0270 - val_decoder_loss: 0.0402 - val_pred_loss: 26.9868 - val_decoder_MAE: 0.1150 - val_pred_corr: 0.0453\n",
      "Epoch 32/100\n",
      "436/436 [==============================] - 10s 22ms/step - loss: 0.6770 - decoder_loss: 0.0671 - pred_loss: 0.6099 - decoder_MAE: 0.1442 - pred_corr: 0.1667 - val_loss: 27.8021 - val_decoder_loss: 0.0386 - val_pred_loss: 27.7635 - val_decoder_MAE: 0.1141 - val_pred_corr: 0.0488\n",
      "Fold 5 val_correlation:\t 0.05693122372031212\n",
      "Weighted Average CV Correlation: 0.068677951892217\n"
     ]
    }
   ],
   "source": [
    "TRAIN=True\n",
    "res_fold = '../results/mlp/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "if not os.path.exists(res_fold):\n",
    "    os.makedirs(res_fold)\n",
    "log_fold = '../logs/mlp/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "if not os.path.exists(log_fold):\n",
    "    os.makedirs(log_fold)\n",
    "\n",
    "\n",
    "if TRAIN:\n",
    "    scores = []\n",
    "    batch_size = 4096\n",
    "    gkf = PurgedGroupTimeSeriesSplit(n_splits = n_splits, group_gap = group_gap)\n",
    "    for fold, (tr, te) in enumerate(gkf.split(X, y, train_data['date'].values)):\n",
    "        ckp_path = res_fold + f'/Model_{fold}.hdf5'\n",
    "        model = create_ae_mlp(**params)\n",
    "        ckp = keras.callbacks.ModelCheckpoint(ckp_path, monitor = 'val_pred_corr', verbose = 0, \n",
    "                              save_best_only = True, save_weights_only = True, mode = 'max')\n",
    "        es = keras.callbacks.EarlyStopping(monitor = 'val_pred_corr', min_delta = 1e-4, patience = 10, mode = 'max', \n",
    "                           baseline = None, restore_best_weights = True, verbose = 0)\n",
    "        tb = keras.callbacks.TensorBoard(log_dir=log_fold+f'/Model_{fold}',\n",
    "                                 histogram_freq=1, write_graph=True, write_images=True, update_freq='epoch',\n",
    "                                 embeddings_freq=0, embeddings_metadata=None) \n",
    "        history = model.fit(X[tr], [X[tr],y[tr]], validation_data = (X[te], [X[te], y[te]]), \n",
    "                            epochs = 100, batch_size = batch_size, callbacks = [ckp, es, tb], verbose = 1)\n",
    "        hist = pd.DataFrame(history.history)\n",
    "        score = hist['val_pred_corr'].max()\n",
    "        print(f'Fold {fold} val_correlation:\\t', score)\n",
    "        scores.append(score)\n",
    "\n",
    "        K.clear_session()\n",
    "        del model\n",
    "        rubbish = gc.collect()\n",
    "    \n",
    "    print('Weighted Average CV Correlation:', np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.057468313723802567,\n",
       " 0.0654989629983902,\n",
       " 0.08434924483299255,\n",
       " 0.07448587566614151,\n",
       " 0.07333409041166306,\n",
       " 0.05693122372031212]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Predict\n",
    "Our goal is to maximize IC(i.e. correlation between y_hat and y) and top 5% group return of predicted signal\n",
    "\n",
    "You need to train a model to predict y for 2 years(2021 & 2022), remember to avoid \"look-ahead bias\"\n",
    "\n",
    "Tips: you can train models in a rolling method\n",
    "\n",
    "i.e. to predict y for 1st week of 2021, you should use all data before 2021\n",
    "\n",
    "but when you try to predict y for 2nd week of 2021, you can train a new model and use data in 1st week of 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000001</th>\n",
       "      <th>000002</th>\n",
       "      <th>000004</th>\n",
       "      <th>000005</th>\n",
       "      <th>000006</th>\n",
       "      <th>000007</th>\n",
       "      <th>000008</th>\n",
       "      <th>000009</th>\n",
       "      <th>000010</th>\n",
       "      <th>000011</th>\n",
       "      <th>...</th>\n",
       "      <th>688786</th>\n",
       "      <th>688787</th>\n",
       "      <th>688788</th>\n",
       "      <th>688789</th>\n",
       "      <th>688793</th>\n",
       "      <th>688798</th>\n",
       "      <th>688799</th>\n",
       "      <th>688800</th>\n",
       "      <th>688819</th>\n",
       "      <th>688981</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-12-26</th>\n",
       "      <td>0.006232</td>\n",
       "      <td>-0.005038</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.035167</td>\n",
       "      <td>0.012865</td>\n",
       "      <td>-0.007457</td>\n",
       "      <td>-0.016356</td>\n",
       "      <td>-0.013902</td>\n",
       "      <td>0.006023</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.013430</td>\n",
       "      <td>-0.018974</td>\n",
       "      <td>-0.009752</td>\n",
       "      <td>-0.009639</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>0.000980</td>\n",
       "      <td>-0.016661</td>\n",
       "      <td>0.000894</td>\n",
       "      <td>-0.007730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-27</th>\n",
       "      <td>-0.015355</td>\n",
       "      <td>-0.021710</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.065904</td>\n",
       "      <td>-0.010698</td>\n",
       "      <td>-0.010017</td>\n",
       "      <td>-0.004844</td>\n",
       "      <td>-0.023718</td>\n",
       "      <td>-0.038799</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011667</td>\n",
       "      <td>0.004526</td>\n",
       "      <td>-0.005614</td>\n",
       "      <td>-0.019048</td>\n",
       "      <td>0.014258</td>\n",
       "      <td>0.002957</td>\n",
       "      <td>0.045801</td>\n",
       "      <td>0.000929</td>\n",
       "      <td>-0.003133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-28</th>\n",
       "      <td>0.012757</td>\n",
       "      <td>0.002152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.001678</td>\n",
       "      <td>0.000916</td>\n",
       "      <td>0.003838</td>\n",
       "      <td>0.002538</td>\n",
       "      <td>0.011509</td>\n",
       "      <td>-0.005254</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004573</td>\n",
       "      <td>0.002386</td>\n",
       "      <td>0.040096</td>\n",
       "      <td>-0.026728</td>\n",
       "      <td>-0.026275</td>\n",
       "      <td>-0.005294</td>\n",
       "      <td>-0.038238</td>\n",
       "      <td>0.006579</td>\n",
       "      <td>0.005155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-29</th>\n",
       "      <td>0.030818</td>\n",
       "      <td>0.002648</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.032339</td>\n",
       "      <td>0.023824</td>\n",
       "      <td>0.021071</td>\n",
       "      <td>0.007014</td>\n",
       "      <td>0.003811</td>\n",
       "      <td>-0.018229</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.038913</td>\n",
       "      <td>0.052655</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>-0.028646</td>\n",
       "      <td>0.018423</td>\n",
       "      <td>0.115483</td>\n",
       "      <td>-0.029777</td>\n",
       "      <td>0.009334</td>\n",
       "      <td>0.003068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-30</th>\n",
       "      <td>0.048962</td>\n",
       "      <td>0.040668</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.022668</td>\n",
       "      <td>-0.003551</td>\n",
       "      <td>0.039103</td>\n",
       "      <td>0.003618</td>\n",
       "      <td>0.005507</td>\n",
       "      <td>0.024574</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.054045</td>\n",
       "      <td>0.030580</td>\n",
       "      <td>-0.012435</td>\n",
       "      <td>0.012652</td>\n",
       "      <td>0.036452</td>\n",
       "      <td>-0.048562</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.003181</td>\n",
       "      <td>0.011284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 4786 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              000001    000002  000004  000005    000006    000007    000008  \\\n",
       "date                                                                           \n",
       "2022-12-26  0.006232 -0.005038     NaN     NaN  0.035167  0.012865 -0.007457   \n",
       "2022-12-27 -0.015355 -0.021710     NaN     NaN -0.065904 -0.010698 -0.010017   \n",
       "2022-12-28  0.012757  0.002152     NaN     NaN -0.001678  0.000916  0.003838   \n",
       "2022-12-29  0.030818  0.002648     NaN     NaN -0.032339  0.023824  0.021071   \n",
       "2022-12-30  0.048962  0.040668     NaN     NaN  0.022668 -0.003551  0.039103   \n",
       "\n",
       "              000009    000010    000011  ...  688786    688787    688788  \\\n",
       "date                                      ...                               \n",
       "2022-12-26 -0.016356 -0.013902  0.006023  ...     NaN -0.013430 -0.018974   \n",
       "2022-12-27 -0.004844 -0.023718 -0.038799  ...     NaN  0.011667  0.004526   \n",
       "2022-12-28  0.002538  0.011509 -0.005254  ...     NaN  0.004573  0.002386   \n",
       "2022-12-29  0.007014  0.003811 -0.018229  ...     NaN  0.038913  0.052655   \n",
       "2022-12-30  0.003618  0.005507  0.024574  ...     NaN  0.054045  0.030580   \n",
       "\n",
       "              688789    688793    688798    688799    688800    688819  \\\n",
       "date                                                                     \n",
       "2022-12-26 -0.009752 -0.009639  0.001342  0.000980 -0.016661  0.000894   \n",
       "2022-12-27 -0.005614 -0.019048  0.014258  0.002957  0.045801  0.000929   \n",
       "2022-12-28  0.040096 -0.026728 -0.026275 -0.005294 -0.038238  0.006579   \n",
       "2022-12-29  0.000578 -0.028646  0.018423  0.115483 -0.029777  0.009334   \n",
       "2022-12-30 -0.012435  0.012652  0.036452 -0.048562  0.001007  0.003181   \n",
       "\n",
       "              688981  \n",
       "date                  \n",
       "2022-12-26 -0.007730  \n",
       "2022-12-27 -0.003133  \n",
       "2022-12-28  0.005155  \n",
       "2022-12-29  0.003068  \n",
       "2022-12-30  0.011284  \n",
       "\n",
       "[5 rows x 4786 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_result=pd.pivot(data,index='date',columns='symbol',values='y')\n",
    "true_result.columns.name=None\n",
    "true_result.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_csv('results/mlp/20230408-234649/mlp_pred_results.csv', index_col=0)\n",
    "true_y = pd.pivot(predictions, index ='date', columns='symbol', values='y')\n",
    "pred_y = pd.pivot(predictions, index ='date', columns='symbol', values='pred_y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try to get better performance than this benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06371427047252856"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calc IC value\n",
    "def IC(fc,stock_return):\n",
    "    ic_series=fc.corrwith(stock_return,axis=1)\n",
    "    return np.mean(ic_series)\n",
    "    \n",
    "IC(pred_y,true_y) # please replace eval_demo by your predicted y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0013175909735005473"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calc top group return\n",
    "def avg_hedge_return(fc,stock_return,group_num=20):\n",
    "    group_mask=fc.apply(lambda x:pd.qcut(x,group_num,labels=False,duplicates='drop'),axis=1)\n",
    "    long_return=stock_return.where(group_mask==group_num-1).mean(axis=1)\n",
    "    market_return=stock_return.mean(axis=1)\n",
    "    hedge_return=long_return-market_return\n",
    "    return np.mean(hedge_return)\n",
    "\n",
    "avg_hedge_return(pred_y,true_y)#please replace eval_demo by your predicted y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
